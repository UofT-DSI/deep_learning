{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba8ab56f",
   "metadata": {},
   "source": [
    "# Advanced Image Classification with ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c630244b8fe2847",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this assignment, you will be asked to develop a convolutional neural network (CNN) to classify images from the CIFAR-100 dataset. At each step, you'll be guided through the process of developing a model architecture to solve a problem. Your goal is to create a CNN that attains at least 55% accuracy on the validation set.\n",
    "\n",
    "### The CIFAR-100 Dataset\n",
    "\n",
    "The [CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) consists of 60000 32x32 colour images in 100 classes, with 600 images per class. There are 50000 training images and 10000 test images. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 500 images from each class.\n",
    "\n",
    "### Tools\n",
    "\n",
    "You will use Keras with TensorFlow to develop your CNN. For this assignment, it's strongly recommended that you use a GPU to accelerate your training, or else you might find it difficult to train your network in a reasonable amount of time. If you have a computer with a GPU that you wish to use, you can follow the [TensorFlow instructions](https://www.tensorflow.org/install/) for installing TensorFlow with GPU support. Otherwise, you can use [Google Colab](https://colab.research.google.com/) to complete this assignment. Colab provides free access to GPU-enabled machines. If you run into any issues, please contact us as soon as possible so that we can help you resolve them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab62988ece1528d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 1: Data Exploration and Preprocessing (Complete or Incomplete)\n",
    "### 1a: Load and Explore the Dataset\n",
    "- Use the code below to download the dataset.\n",
    "- Explore the dataset: examine the shape of the training and test sets, the dimensions of the images, and the number of classes. Show a few examples from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8820fcdc5ae52ae2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-26T17:04:08.432758Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar100\n",
    "\n",
    "# Load the CIFAR-100 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a386b4072078138f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    \"apple\", \"aquarium_fish\", \"baby\", \"bear\", \"beaver\", \"bed\", \"bee\", \"beetle\", \"bicycle\", \"bottle\",\n",
    "    \"bowl\", \"boy\", \"bridge\", \"bus\", \"butterfly\", \"camel\", \"can\", \"castle\", \"caterpillar\", \"cattle\",\n",
    "    \"chair\", \"chimpanzee\", \"clock\", \"cloud\", \"cockroach\", \"couch\", \"crab\", \"crocodile\", \"cup\", \"dinosaur\",\n",
    "    \"dolphin\", \"elephant\", \"flatfish\", \"forest\", \"fox\", \"girl\", \"hamster\", \"house\", \"kangaroo\", \"keyboard\",\n",
    "    \"lamp\", \"lawn_mower\", \"leopard\", \"lion\", \"lizard\", \"lobster\", \"man\", \"maple_tree\", \"motorcycle\", \"mountain\",\n",
    "    \"mouse\", \"mushroom\", \"oak_tree\", \"orange\", \"orchid\", \"otter\", \"palm_tree\", \"pear\", \"pickup_truck\", \"pine_tree\",\n",
    "    \"plain\", \"plate\", \"poppy\", \"porcupine\", \"possum\", \"rabbit\", \"raccoon\", \"ray\", \"road\", \"rocket\",\n",
    "    \"rose\", \"sea\", \"seal\", \"shark\", \"shrew\", \"skunk\", \"skyscraper\", \"snail\", \"snake\", \"spider\",\n",
    "    \"squirrel\", \"streetcar\", \"sunflower\", \"sweet_pepper\", \"table\", \"tank\", \"telephone\", \"television\", \"tiger\", \"tractor\",\n",
    "    \"train\", \"trout\", \"tulip\", \"turtle\", \"wardrobe\", \"whale\", \"willow_tree\", \"wolf\", \"woman\", \"worm\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36ca8354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 1)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Examine the shape \n",
    "import pandas as pd\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d80bd996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n",
      "int32\n"
     ]
    }
   ],
   "source": [
    "# Look at the data types \n",
    "print(x_train.dtype)\n",
    "print(y_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31684140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[255 255 255 255 255]\n",
      " [255 254 254 254 254]\n",
      " [255 254 255 255 255]\n",
      " [255 254 255 255 255]\n",
      " [255 254 255 255 255]]\n",
      "[[19]\n",
      " [29]\n",
      " [ 0]\n",
      " ...\n",
      " [ 3]\n",
      " [ 7]\n",
      " [73]]\n"
     ]
    }
   ],
   "source": [
    "# Look at sample values \n",
    "print(x_train[0, :5, :5, 0])  \n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6478aaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n"
     ]
    }
   ],
   "source": [
    "# Find the unique values \n",
    "import numpy as np\n",
    "\n",
    "unique_values_x = np.unique(x_train)\n",
    "print(unique_values_x)\n",
    "unique_values_y = np.unique(y_train)\n",
    "print(unique_values_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad49291da3a819ea",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1b: Data Preprocessing\n",
    "- With the data downloaded, it's time to preprocess it. Start by normalizing the images so that they all have pixel values in the range [0, 1].\n",
    "- Next, convert the labels to one-hot encoded vectors.\n",
    "- Finally, split the training set into training and validation sets. Use 80% of the training set for training and the remaining 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72640a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [0.7647059  0.8039216  0.75686276]\n",
      "   [0.83137256 0.8784314  0.8       ]\n",
      "   [0.7137255  0.7607843  0.654902  ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   ...\n",
      "   [0.6666667  0.6901961  0.5882353 ]\n",
      "   [0.6313726  0.65882355 0.50980395]\n",
      "   [0.57254905 0.6039216  0.44313726]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [0.7411765  0.78039217 0.6627451 ]\n",
      "   [0.6509804  0.69803923 0.50980395]\n",
      "   [0.4745098  0.52156866 0.34117648]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.5803922  0.7254902  0.30980393]\n",
      "   [0.5568628  0.7137255  0.22352941]\n",
      "   [0.54901963 0.7019608  0.23529412]\n",
      "   ...\n",
      "   [0.11764706 0.06666667 0.00392157]\n",
      "   [0.25490198 0.24313726 0.05882353]\n",
      "   [0.29803923 0.3019608  0.07843138]]\n",
      "\n",
      "  [[0.47843137 0.6156863  0.25882354]\n",
      "   [0.47058824 0.60784316 0.22745098]\n",
      "   [0.49411765 0.627451   0.2784314 ]\n",
      "   ...\n",
      "   [0.08627451 0.0627451  0.01176471]\n",
      "   [0.38039216 0.4392157  0.21960784]\n",
      "   [0.5529412  0.6313726  0.34117648]]\n",
      "\n",
      "  [[0.34117648 0.47843137 0.16078432]\n",
      "   [0.34509805 0.47843137 0.15294118]\n",
      "   [0.39607844 0.5254902  0.21960784]\n",
      "   ...\n",
      "   [0.13333334 0.14117648 0.03921569]\n",
      "   [0.4117647  0.52156866 0.23137255]\n",
      "   [0.5411765  0.6784314  0.30980393]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.        ]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   ...\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   ...\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   [1.         1.         1.        ]]]\n",
      "\n",
      "\n",
      " [[[0.98039216 0.98039216 0.972549  ]\n",
      "   [0.972549   0.9764706  0.9529412 ]\n",
      "   [0.96862745 0.972549   0.9372549 ]\n",
      "   ...\n",
      "   [0.98039216 0.98039216 0.9647059 ]\n",
      "   [0.98039216 0.98039216 0.9647059 ]\n",
      "   [0.9764706  0.98039216 0.9647059 ]]\n",
      "\n",
      "  [[0.98039216 0.9843137  0.9607843 ]\n",
      "   [0.972549   0.9764706  0.93333334]\n",
      "   [0.96862745 0.96862745 0.91764706]\n",
      "   ...\n",
      "   [0.9843137  0.9843137  0.9490196 ]\n",
      "   [0.9843137  0.9882353  0.9529412 ]\n",
      "   [0.98039216 0.9843137  0.9529412 ]]\n",
      "\n",
      "  [[0.9843137  0.9843137  0.95686275]\n",
      "   [0.98039216 0.972549   0.92941177]\n",
      "   [0.98039216 0.9607843  0.9137255 ]\n",
      "   ...\n",
      "   [0.98039216 0.9764706  0.93333334]\n",
      "   [0.98039216 0.9764706  0.9411765 ]\n",
      "   [0.98039216 0.9764706  0.9490196 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8666667  0.8352941  0.7490196 ]\n",
      "   [0.8666667  0.80784315 0.6901961 ]\n",
      "   [0.88235295 0.8117647  0.70980394]\n",
      "   ...\n",
      "   [0.78039217 0.6901961  0.5254902 ]\n",
      "   [0.8117647  0.75686276 0.64705884]\n",
      "   [0.9137255  0.8980392  0.8862745 ]]\n",
      "\n",
      "  [[0.88235295 0.8745098  0.8       ]\n",
      "   [0.8901961  0.85882354 0.76862746]\n",
      "   [0.8980392  0.84705883 0.78431374]\n",
      "   ...\n",
      "   [0.8        0.7254902  0.5921569 ]\n",
      "   [0.83137256 0.7882353  0.7058824 ]\n",
      "   [0.91764706 0.9098039  0.89411765]]\n",
      "\n",
      "  [[0.9137255  0.9137255  0.8862745 ]\n",
      "   [0.91764706 0.9098039  0.8784314 ]\n",
      "   [0.92156863 0.9019608  0.88235295]\n",
      "   ...\n",
      "   [0.85882354 0.81960785 0.7607843 ]\n",
      "   [0.8745098  0.84705883 0.8117647 ]\n",
      "   [0.9098039  0.9019608  0.89411765]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.972549   0.95686275 0.9490196 ]\n",
      "   [0.9411765  0.9098039  0.8745098 ]\n",
      "   [0.9254902  0.9098039  0.8745098 ]\n",
      "   ...\n",
      "   [0.9137255  0.8980392  0.87058824]\n",
      "   [0.9019608  0.89411765 0.87058824]\n",
      "   [0.93333334 0.92941177 0.9137255 ]]\n",
      "\n",
      "  [[0.88235295 0.8352941  0.8       ]\n",
      "   [0.7294118  0.654902   0.58431375]\n",
      "   [0.6862745  0.62352943 0.54901963]\n",
      "   ...\n",
      "   [0.6392157  0.5803922  0.5254902 ]\n",
      "   [0.6117647  0.5647059  0.52156866]\n",
      "   [0.7529412  0.72156864 0.6901961 ]]\n",
      "\n",
      "  [[0.81960785 0.7607843  0.7019608 ]\n",
      "   [0.5647059  0.47058824 0.37254903]\n",
      "   [0.54509807 0.4509804  0.34117648]\n",
      "   ...\n",
      "   [0.42745098 0.3372549  0.2627451 ]\n",
      "   [0.42745098 0.3529412  0.29803923]\n",
      "   [0.6156863  0.5686275  0.5294118 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6313726  0.62352943 0.6       ]\n",
      "   [0.15294118 0.13333334 0.10980392]\n",
      "   [0.10980392 0.07843138 0.05490196]\n",
      "   ...\n",
      "   [0.3647059  0.28235295 0.20784314]\n",
      "   [0.33333334 0.2627451  0.19607843]\n",
      "   [0.53333336 0.49411765 0.4509804 ]]\n",
      "\n",
      "  [[0.70980394 0.7019608  0.6745098 ]\n",
      "   [0.3372549  0.3254902  0.3019608 ]\n",
      "   [0.2784314  0.26666668 0.24313726]\n",
      "   ...\n",
      "   [0.47843137 0.40392157 0.34901962]\n",
      "   [0.4117647  0.36078432 0.32156864]\n",
      "   [0.5921569  0.5686275  0.5529412 ]]\n",
      "\n",
      "  [[0.8784314  0.8745098  0.85490197]\n",
      "   [0.7058824  0.7058824  0.6862745 ]\n",
      "   [0.6784314  0.6745098  0.654902  ]\n",
      "   ...\n",
      "   [0.76862746 0.73333335 0.7058824 ]\n",
      "   [0.7176471  0.69803923 0.68235296]\n",
      "   [0.8        0.8039216  0.8039216 ]]]\n",
      "\n",
      "\n",
      " [[[0.6117647  0.6039216  0.5372549 ]\n",
      "   [0.5921569  0.57254905 0.48235294]\n",
      "   [0.5921569  0.5647059  0.49019608]\n",
      "   ...\n",
      "   [0.60784316 0.5882353  0.5058824 ]\n",
      "   [0.59607846 0.5803922  0.49019608]\n",
      "   [0.7294118  0.72156864 0.6392157 ]]\n",
      "\n",
      "  [[0.43137255 0.41568628 0.3019608 ]\n",
      "   [0.45490196 0.42352942 0.24313726]\n",
      "   [0.44705883 0.39607844 0.22352941]\n",
      "   ...\n",
      "   [0.45490196 0.41568628 0.23921569]\n",
      "   [0.43529412 0.40392157 0.21960784]\n",
      "   [0.5254902  0.5058824  0.36078432]]\n",
      "\n",
      "  [[0.45490196 0.4392157  0.32156864]\n",
      "   [0.4862745  0.4627451  0.25882354]\n",
      "   [0.5019608  0.4627451  0.2627451 ]\n",
      "   ...\n",
      "   [0.3882353  0.32941177 0.16862746]\n",
      "   [0.39607844 0.34117648 0.16862746]\n",
      "   [0.5058824  0.4627451  0.3372549 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.42745098 0.39607844 0.2901961 ]\n",
      "   [0.4392157  0.39215687 0.21176471]\n",
      "   [0.4627451  0.4117647  0.24313726]\n",
      "   ...\n",
      "   [0.49411765 0.44313726 0.25490198]\n",
      "   [0.49411765 0.43529412 0.23921569]\n",
      "   [0.5411765  0.4862745  0.34901962]]\n",
      "\n",
      "  [[0.38431373 0.36078432 0.24705882]\n",
      "   [0.3647059  0.32156864 0.13725491]\n",
      "   [0.3764706  0.3254902  0.14901961]\n",
      "   ...\n",
      "   [0.4392157  0.3764706  0.18431373]\n",
      "   [0.42745098 0.36078432 0.1764706 ]\n",
      "   [0.49803922 0.44313726 0.3137255 ]]\n",
      "\n",
      "  [[0.6666667  0.654902   0.5686275 ]\n",
      "   [0.627451   0.6        0.4627451 ]\n",
      "   [0.6392157  0.59607846 0.46666667]\n",
      "   ...\n",
      "   [0.6313726  0.5921569  0.44705883]\n",
      "   [0.6117647  0.5647059  0.41960785]\n",
      "   [0.6392157  0.6039216  0.49411765]]]\n",
      "\n",
      "\n",
      " [[[0.12156863 0.2627451  0.47843137]\n",
      "   [0.11764706 0.26666668 0.4862745 ]\n",
      "   [0.12156863 0.27058825 0.49411765]\n",
      "   ...\n",
      "   [0.1254902  0.27450982 0.5058824 ]\n",
      "   [0.1254902  0.27450982 0.49019608]\n",
      "   [0.1254902  0.27058825 0.47843137]]\n",
      "\n",
      "  [[0.11372549 0.26666668 0.49411765]\n",
      "   [0.10980392 0.27058825 0.5019608 ]\n",
      "   [0.11764706 0.27058825 0.50980395]\n",
      "   ...\n",
      "   [0.1254902  0.27450982 0.5137255 ]\n",
      "   [0.1254902  0.27058825 0.49803922]\n",
      "   [0.12156863 0.27058825 0.4862745 ]]\n",
      "\n",
      "  [[0.11764706 0.2627451  0.49411765]\n",
      "   [0.11372549 0.26666668 0.5019608 ]\n",
      "   [0.11764706 0.27058825 0.50980395]\n",
      "   ...\n",
      "   [0.1254902  0.28235295 0.5176471 ]\n",
      "   [0.12156863 0.27450982 0.50980395]\n",
      "   [0.11764706 0.27058825 0.49803922]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.15294118 0.16078432 0.29803923]\n",
      "   [0.14901961 0.16470589 0.29803923]\n",
      "   [0.14901961 0.17254902 0.30588236]\n",
      "   ...\n",
      "   [0.15294118 0.17254902 0.30980393]\n",
      "   [0.14901961 0.16470589 0.3019608 ]\n",
      "   [0.15294118 0.16078432 0.29803923]]\n",
      "\n",
      "  [[0.15686275 0.15294118 0.28627452]\n",
      "   [0.15294118 0.15686275 0.2901961 ]\n",
      "   [0.15294118 0.16078432 0.29803923]\n",
      "   ...\n",
      "   [0.15294118 0.16078432 0.29803923]\n",
      "   [0.15686275 0.16078432 0.2901961 ]\n",
      "   [0.15686275 0.15294118 0.28627452]]\n",
      "\n",
      "  [[0.15686275 0.15294118 0.27450982]\n",
      "   [0.15686275 0.15294118 0.2784314 ]\n",
      "   [0.15686275 0.15294118 0.28235295]\n",
      "   ...\n",
      "   [0.16078432 0.14901961 0.28235295]\n",
      "   [0.15294118 0.14901961 0.27058825]\n",
      "   [0.15686275 0.14509805 0.2627451 ]]]]\n",
      "[[[[0.78039217 0.84313726 0.9764706 ]\n",
      "   [0.76862746 0.827451   0.95686275]\n",
      "   [0.7647059  0.8235294  0.9529412 ]\n",
      "   ...\n",
      "   [0.84705883 0.90588236 0.98039216]\n",
      "   [0.8509804  0.90588236 0.98039216]\n",
      "   [0.8784314  0.91764706 0.9882353 ]]\n",
      "\n",
      "  [[0.77254903 0.8235294  0.9372549 ]\n",
      "   [0.7647059  0.8156863  0.93333334]\n",
      "   [0.7647059  0.8235294  0.9411765 ]\n",
      "   ...\n",
      "   [0.90588236 0.9529412  0.98039216]\n",
      "   [0.9137255  0.9529412  0.98039216]\n",
      "   [0.94509804 0.9607843  0.99215686]]\n",
      "\n",
      "  [[0.87058824 0.8862745  0.9647059 ]\n",
      "   [0.8352941  0.8627451  0.9490196 ]\n",
      "   [0.81960785 0.85882354 0.9529412 ]\n",
      "   ...\n",
      "   [0.9529412  0.98039216 0.9843137 ]\n",
      "   [0.95686275 0.9764706  0.9843137 ]\n",
      "   [0.98039216 0.98039216 0.99215686]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.28235295 0.28627452 0.3882353 ]\n",
      "   [0.2784314  0.2901961  0.4       ]\n",
      "   [0.2901961  0.30588236 0.42352942]\n",
      "   ...\n",
      "   [0.8627451  0.8156863  0.8509804 ]\n",
      "   [0.7176471  0.65882355 0.70980394]\n",
      "   [0.60784316 0.5529412  0.5882353 ]]\n",
      "\n",
      "  [[0.28235295 0.29411766 0.40784314]\n",
      "   [0.29803923 0.31764707 0.43529412]\n",
      "   [0.32941177 0.34901962 0.47843137]\n",
      "   ...\n",
      "   [0.87058824 0.83137256 0.8627451 ]\n",
      "   [0.73333335 0.68235296 0.7529412 ]\n",
      "   [0.5686275  0.5176471  0.58431375]]\n",
      "\n",
      "  [[0.3137255  0.33333334 0.4627451 ]\n",
      "   [0.32941177 0.3529412  0.48235294]\n",
      "   [0.33333334 0.36078432 0.49803922]\n",
      "   ...\n",
      "   [0.8509804  0.8117647  0.84313726]\n",
      "   [0.8117647  0.7607843  0.827451  ]\n",
      "   [0.6901961  0.6431373  0.7176471 ]]]\n",
      "\n",
      "\n",
      " [[[0.44313726 0.50980395 0.38431373]\n",
      "   [0.34509805 0.4117647  0.28627452]\n",
      "   [0.28235295 0.34901962 0.22745098]\n",
      "   ...\n",
      "   [0.4117647  0.4862745  0.3647059 ]\n",
      "   [0.3372549  0.41568628 0.2901961 ]\n",
      "   [0.24705882 0.32156864 0.21568628]]\n",
      "\n",
      "  [[0.37254903 0.44313726 0.3137255 ]\n",
      "   [0.30588236 0.38039216 0.2509804 ]\n",
      "   [0.24313726 0.3137255  0.19215687]\n",
      "   ...\n",
      "   [0.40392157 0.47843137 0.32941177]\n",
      "   [0.34901962 0.42745098 0.27450982]\n",
      "   [0.25490198 0.3254902  0.19215687]]\n",
      "\n",
      "  [[0.30980393 0.38039216 0.24313726]\n",
      "   [0.25490198 0.32156864 0.19215687]\n",
      "   [0.20392157 0.2627451  0.14509805]\n",
      "   ...\n",
      "   [0.25490198 0.30980393 0.20392157]\n",
      "   [0.24313726 0.30980393 0.2       ]\n",
      "   [0.19215687 0.2509804  0.14901961]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.4745098  0.4        0.3372549 ]\n",
      "   [0.49803922 0.42352942 0.3764706 ]\n",
      "   [0.54509807 0.4745098  0.43529412]\n",
      "   ...\n",
      "   [0.3137255  0.39215687 0.2784314 ]\n",
      "   [0.29411766 0.37254903 0.26666668]\n",
      "   [0.25882354 0.33333334 0.24313726]]\n",
      "\n",
      "  [[0.6666667  0.5686275  0.5137255 ]\n",
      "   [0.654902   0.5686275  0.5176471 ]\n",
      "   [0.62352943 0.54901963 0.5019608 ]\n",
      "   ...\n",
      "   [0.39215687 0.44313726 0.32941177]\n",
      "   [0.34901962 0.40392157 0.3019608 ]\n",
      "   [0.28627452 0.34117648 0.2509804 ]]\n",
      "\n",
      "  [[0.65882355 0.5882353  0.5294118 ]\n",
      "   [0.6666667  0.59607846 0.5372549 ]\n",
      "   [0.6784314  0.6117647  0.5529412 ]\n",
      "   ...\n",
      "   [0.4627451  0.48235294 0.3764706 ]\n",
      "   [0.3882353  0.41568628 0.3137255 ]\n",
      "   [0.32941177 0.36078432 0.2627451 ]]]\n",
      "\n",
      "\n",
      " [[[0.23921569 0.3529412  0.47058824]\n",
      "   [0.23921569 0.34901962 0.46666667]\n",
      "   [0.2627451  0.3764706  0.49019608]\n",
      "   ...\n",
      "   [0.27450982 0.3647059  0.4862745 ]\n",
      "   [0.2509804  0.34117648 0.4627451 ]\n",
      "   [0.2509804  0.34509805 0.46666667]]\n",
      "\n",
      "  [[0.2627451  0.38039216 0.49803922]\n",
      "   [0.30588236 0.42352942 0.5411765 ]\n",
      "   [0.3137255  0.42745098 0.54509807]\n",
      "   ...\n",
      "   [0.37254903 0.44313726 0.5568628 ]\n",
      "   [0.3254902  0.39607844 0.5058824 ]\n",
      "   [0.3254902  0.4        0.50980395]]\n",
      "\n",
      "  [[0.2509804  0.3647059  0.48235294]\n",
      "   [0.25882354 0.37254903 0.49019608]\n",
      "   [0.23921569 0.3529412  0.47058824]\n",
      "   ...\n",
      "   [0.47058824 0.5254902  0.627451  ]\n",
      "   [0.42745098 0.48235294 0.5882353 ]\n",
      "   [0.4392157  0.49411765 0.59607846]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.32941177 0.34117648 0.34117648]\n",
      "   [0.2627451  0.23137255 0.14901961]\n",
      "   [0.32941177 0.30588236 0.12941177]\n",
      "   ...\n",
      "   [0.8        0.654902   0.627451  ]\n",
      "   [0.92156863 0.7921569  0.7529412 ]\n",
      "   [0.9254902  0.80784315 0.75686276]]\n",
      "\n",
      "  [[0.28235295 0.2509804  0.21176471]\n",
      "   [0.27450982 0.21176471 0.11764706]\n",
      "   [0.32941177 0.30980393 0.15686275]\n",
      "   ...\n",
      "   [0.78039217 0.6156863  0.59607846]\n",
      "   [0.74509805 0.5882353  0.5568628 ]\n",
      "   [0.76862746 0.6313726  0.58431375]]\n",
      "\n",
      "  [[0.28235295 0.23921569 0.18431373]\n",
      "   [0.29803923 0.2509804  0.13725491]\n",
      "   [0.3019608  0.2901961  0.14509805]\n",
      "   ...\n",
      "   [0.8901961  0.7490196  0.7411765 ]\n",
      "   [0.60784316 0.46666667 0.4509804 ]\n",
      "   [0.5254902  0.38431373 0.34901962]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.09411765 0.14901961 0.12156863]\n",
      "   [0.09411765 0.15686275 0.12156863]\n",
      "   [0.08627451 0.13333334 0.10980392]\n",
      "   ...\n",
      "   [0.21960784 0.23137255 0.21568628]\n",
      "   [0.13725491 0.15686275 0.14509805]\n",
      "   [0.09803922 0.13725491 0.1254902 ]]\n",
      "\n",
      "  [[0.07450981 0.11372549 0.10196079]\n",
      "   [0.10980392 0.1764706  0.13725491]\n",
      "   [0.09803922 0.15686275 0.11372549]\n",
      "   ...\n",
      "   [0.21568628 0.22352941 0.22745098]\n",
      "   [0.14901961 0.16470589 0.17254902]\n",
      "   [0.14117648 0.17254902 0.15294118]]\n",
      "\n",
      "  [[0.09019608 0.14901961 0.10980392]\n",
      "   [0.12156863 0.2        0.13725491]\n",
      "   [0.11372549 0.19607843 0.1254902 ]\n",
      "   ...\n",
      "   [0.12941177 0.16078432 0.14117648]\n",
      "   [0.12941177 0.14509805 0.16078432]\n",
      "   [0.16470589 0.19215687 0.17254902]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.15294118 0.28627452 0.11372549]\n",
      "   [0.18431373 0.37254903 0.09803922]\n",
      "   [0.23921569 0.40784314 0.17254902]\n",
      "   ...\n",
      "   [0.16078432 0.30980393 0.13725491]\n",
      "   [0.2784314  0.40392157 0.28235295]\n",
      "   [0.21176471 0.3647059  0.20784314]]\n",
      "\n",
      "  [[0.15294118 0.28235295 0.13333334]\n",
      "   [0.15686275 0.35686275 0.07058824]\n",
      "   [0.2        0.3764706  0.12941177]\n",
      "   ...\n",
      "   [0.12941177 0.27450982 0.09411765]\n",
      "   [0.2627451  0.39215687 0.24313726]\n",
      "   [0.23921569 0.39607844 0.21568628]]\n",
      "\n",
      "  [[0.1882353  0.34901962 0.12941177]\n",
      "   [0.14509805 0.3254902  0.06666667]\n",
      "   [0.18039216 0.35686275 0.09411765]\n",
      "   ...\n",
      "   [0.12941177 0.25882354 0.07843138]\n",
      "   [0.25490198 0.37254903 0.21176471]\n",
      "   [0.23529412 0.36862746 0.21176471]]]\n",
      "\n",
      "\n",
      " [[[0.3372549  0.3529412  0.3137255 ]\n",
      "   [0.3647059  0.3529412  0.29411766]\n",
      "   [0.10980392 0.09803922 0.08235294]\n",
      "   ...\n",
      "   [0.28627452 0.20392157 0.14117648]\n",
      "   [0.2627451  0.19215687 0.15294118]\n",
      "   [0.36078432 0.2784314  0.23921569]]\n",
      "\n",
      "  [[0.32156864 0.3372549  0.3137255 ]\n",
      "   [0.16862746 0.17254902 0.16078432]\n",
      "   [0.06666667 0.06666667 0.07058824]\n",
      "   ...\n",
      "   [0.25490198 0.16470589 0.10980392]\n",
      "   [0.13333334 0.09019608 0.0627451 ]\n",
      "   [0.21960784 0.16078432 0.12941177]]\n",
      "\n",
      "  [[0.2509804  0.25882354 0.23529412]\n",
      "   [0.09803922 0.10588235 0.10196079]\n",
      "   [0.15294118 0.15294118 0.15294118]\n",
      "   ...\n",
      "   [0.21176471 0.14509805 0.09411765]\n",
      "   [0.13333334 0.09803922 0.07450981]\n",
      "   [0.15686275 0.12156863 0.08627451]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.16862746 0.16862746 0.15294118]\n",
      "   [0.34117648 0.32156864 0.27450982]\n",
      "   [0.4392157  0.4117647  0.35686275]\n",
      "   ...\n",
      "   [0.50980395 0.50980395 0.48235294]\n",
      "   [0.5019608  0.49411765 0.4745098 ]\n",
      "   [0.46666667 0.4745098  0.46666667]]\n",
      "\n",
      "  [[0.17254902 0.16470589 0.13725491]\n",
      "   [0.30980393 0.2901961  0.24313726]\n",
      "   [0.3882353  0.37254903 0.32156864]\n",
      "   ...\n",
      "   [0.5294118  0.52156866 0.48235294]\n",
      "   [0.50980395 0.49803922 0.46666667]\n",
      "   [0.4745098  0.47058824 0.44705883]]\n",
      "\n",
      "  [[0.33333334 0.32156864 0.27450982]\n",
      "   [0.38039216 0.35686275 0.3019608 ]\n",
      "   [0.3529412  0.34509805 0.29803923]\n",
      "   ...\n",
      "   [0.5176471  0.5019608  0.46666667]\n",
      "   [0.5058824  0.4862745  0.4509804 ]\n",
      "   [0.47058824 0.45882353 0.42745098]]]\n",
      "\n",
      "\n",
      " [[[0.9647059  0.9647059  0.9490196 ]\n",
      "   [0.9411765  0.93333334 0.9098039 ]\n",
      "   [0.8392157  0.83137256 0.78039217]\n",
      "   ...\n",
      "   [0.2901961  0.1254902  0.13725491]\n",
      "   [0.3019608  0.13333334 0.14509805]\n",
      "   [0.31764707 0.13333334 0.13725491]]\n",
      "\n",
      "  [[0.8235294  0.8039216  0.76862746]\n",
      "   [0.9529412  0.9411765  0.9019608 ]\n",
      "   [0.8980392  0.88235295 0.8392157 ]\n",
      "   ...\n",
      "   [0.29411766 0.12941177 0.13725491]\n",
      "   [0.30980393 0.13725491 0.14901961]\n",
      "   [0.3254902  0.13333334 0.14117648]]\n",
      "\n",
      "  [[0.5647059  0.5254902  0.4392157 ]\n",
      "   [0.6862745  0.6392157  0.5647059 ]\n",
      "   [0.61960787 0.5647059  0.50980395]\n",
      "   ...\n",
      "   [0.2901961  0.12941177 0.13725491]\n",
      "   [0.30980393 0.13725491 0.14901961]\n",
      "   [0.32156864 0.12941177 0.14117648]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.7764706  0.74509805 0.6901961 ]\n",
      "   [0.43529412 0.43529412 0.25882354]\n",
      "   [0.22745098 0.21568628 0.10588235]\n",
      "   ...\n",
      "   [0.24313726 0.31764707 0.16078432]\n",
      "   [0.28235295 0.39215687 0.16078432]\n",
      "   [0.3137255  0.41960785 0.19215687]]\n",
      "\n",
      "  [[0.654902   0.627451   0.5647059 ]\n",
      "   [0.24313726 0.2509804  0.10588235]\n",
      "   [0.33333334 0.33333334 0.26666668]\n",
      "   ...\n",
      "   [0.36078432 0.49411765 0.22745098]\n",
      "   [0.56078434 0.7176471  0.40784314]\n",
      "   [0.627451   0.78039217 0.4627451 ]]\n",
      "\n",
      "  [[0.4509804  0.42352942 0.36862746]\n",
      "   [0.16470589 0.14509805 0.08235294]\n",
      "   [0.54509807 0.53333336 0.49803922]\n",
      "   ...\n",
      "   [0.54509807 0.6745098  0.44705883]\n",
      "   [0.654902   0.8        0.5529412 ]\n",
      "   [0.57254905 0.7137255  0.4627451 ]]]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the images \n",
    "x_train_n = x_train.astype('float32') / 255.0\n",
    "x_test_n = x_test.astype('float32') / 255.0\n",
    "print(x_train_n)\n",
    "print(x_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef8a0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Perform one hot encoding on the labels \n",
    "y_train_oh = to_categorical(y_train, num_classes=100)\n",
    "y_test_oh = to_categorical(y_test, num_classes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "371347ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into training and validation sets \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_new, x_test_new, y_train_new, y_test_new = train_test_split(\n",
    "    x_train_n, \n",
    "    y_train_oh,\n",
    "    test_size=0.2, # 20% of the data is used for testing\n",
    "    random_state=42 # Providing a value here means getting the same \"random\" split every time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da7422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000, 100)\n",
      "x_test shape: (10000, 32, 32, 3)\n",
      "y_test shape: (10000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Verify the data has been split correctly \n",
    "print(f'x_train shape: {x_train_new.shape}')\n",
    "print(f'y_train shape: {y_train_new.shape}')\n",
    "print(f'x_test shape: {x_test_new.shape}')\n",
    "print(f'y_test shape: {y_test_new.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993757f08c89db7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 2: Model Development (Complete or Incomplete)\n",
    "### Task 2a: Create a Baseline CNN Model\n",
    "- Design a CNN architecture. Your architecture should use convolutional layers, max pooling layers, and dense layers. You can use any number of layers, and you can experiment with different numbers of filters, filter sizes, strides, padding, etc. The design doesn't need to be perfect, but it should be unique to you.\n",
    "- Print out the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9edafdaf887b8d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">230,500</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │       \u001b[38;5;34m230,500\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">249,892</span> (976.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m249,892\u001b[0m (976.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">249,892</span> (976.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m249,892\u001b[0m (976.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a CNN model \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add convolution layers \n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Add fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546324c007c73db5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 2b: Compile the model\n",
    "\n",
    "- Select an appropriate loss function and optimizer for your model. These can be ones we have looked at already, or they can be different. \n",
    "- Briefly explain your choices (one or two sentences each).\n",
    "- <b>Loss function:</b> ______\n",
    "- <b>Optimizer:</b> ______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ab39f4ba69d684e9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import TopKCategoricalAccuracy\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    ")\n",
    "\n",
    "# The loss function categorical crossentropy was selected because this is a multi-class classification problem.\n",
    "# The Adam optimizer was selected due to the size of the dataset and for its faster convergence time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653fba928413b9f6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 3: Model Training and Evaluation (Complete or Incomplete)\n",
    "### Task 3a: Train the Model\n",
    "\n",
    "- Train your model for an appropriate number of epochs. Explain your choice of the number of epochs used - you can change this number before submitting your assignment.\n",
    "- Use a batch size of 32.\n",
    "- Use the validation set for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9de74f274ad08546",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.1731 - loss: 3.4732 - top_5_accuracy: 0.4366 - val_accuracy: 0.2457 - val_loss: 3.0891 - val_top_5_accuracy: 0.5354\n",
      "Epoch 2/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.2906 - loss: 2.8536 - top_5_accuracy: 0.5894 - val_accuracy: 0.3222 - val_loss: 2.7160 - val_top_5_accuracy: 0.6262\n",
      "Epoch 3/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 29ms/step - accuracy: 0.3621 - loss: 2.5079 - top_5_accuracy: 0.6669 - val_accuracy: 0.3493 - val_loss: 2.5973 - val_top_5_accuracy: 0.6493\n",
      "Epoch 4/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.4161 - loss: 2.2629 - top_5_accuracy: 0.7208 - val_accuracy: 0.3619 - val_loss: 2.5664 - val_top_5_accuracy: 0.6641\n",
      "Epoch 5/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 24ms/step - accuracy: 0.4546 - loss: 2.0562 - top_5_accuracy: 0.7589 - val_accuracy: 0.3752 - val_loss: 2.5065 - val_top_5_accuracy: 0.6706\n",
      "Epoch 6/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 24ms/step - accuracy: 0.5019 - loss: 1.8590 - top_5_accuracy: 0.8013 - val_accuracy: 0.3672 - val_loss: 2.5661 - val_top_5_accuracy: 0.6658\n",
      "Epoch 7/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 24ms/step - accuracy: 0.5347 - loss: 1.6999 - top_5_accuracy: 0.8259 - val_accuracy: 0.3741 - val_loss: 2.5953 - val_top_5_accuracy: 0.6664\n",
      "Epoch 8/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 24ms/step - accuracy: 0.5662 - loss: 1.5572 - top_5_accuracy: 0.8544 - val_accuracy: 0.3731 - val_loss: 2.6261 - val_top_5_accuracy: 0.6687\n",
      "Epoch 9/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 23ms/step - accuracy: 0.6024 - loss: 1.4151 - top_5_accuracy: 0.8748 - val_accuracy: 0.3706 - val_loss: 2.7074 - val_top_5_accuracy: 0.6654\n",
      "Epoch 10/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 24ms/step - accuracy: 0.6251 - loss: 1.3051 - top_5_accuracy: 0.8938 - val_accuracy: 0.3702 - val_loss: 2.8107 - val_top_5_accuracy: 0.6603\n",
      "Epoch 11/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 24ms/step - accuracy: 0.6488 - loss: 1.2114 - top_5_accuracy: 0.9100 - val_accuracy: 0.3708 - val_loss: 2.9186 - val_top_5_accuracy: 0.6586\n",
      "Epoch 12/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 24ms/step - accuracy: 0.6720 - loss: 1.1295 - top_5_accuracy: 0.9202 - val_accuracy: 0.3713 - val_loss: 2.9443 - val_top_5_accuracy: 0.6610\n",
      "Epoch 13/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 24ms/step - accuracy: 0.6932 - loss: 1.0321 - top_5_accuracy: 0.9326 - val_accuracy: 0.3703 - val_loss: 3.0598 - val_top_5_accuracy: 0.6572\n",
      "Epoch 14/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 24ms/step - accuracy: 0.7096 - loss: 0.9686 - top_5_accuracy: 0.9400 - val_accuracy: 0.3557 - val_loss: 3.2259 - val_top_5_accuracy: 0.6469\n",
      "Epoch 15/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 24ms/step - accuracy: 0.7288 - loss: 0.8979 - top_5_accuracy: 0.9490 - val_accuracy: 0.3680 - val_loss: 3.2615 - val_top_5_accuracy: 0.6520\n",
      "Epoch 16/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 23ms/step - accuracy: 0.7506 - loss: 0.8295 - top_5_accuracy: 0.9549 - val_accuracy: 0.3679 - val_loss: 3.4456 - val_top_5_accuracy: 0.6491\n",
      "Epoch 17/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 24ms/step - accuracy: 0.7597 - loss: 0.8009 - top_5_accuracy: 0.9586 - val_accuracy: 0.3638 - val_loss: 3.4345 - val_top_5_accuracy: 0.6399\n",
      "Epoch 18/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 26ms/step - accuracy: 0.7723 - loss: 0.7443 - top_5_accuracy: 0.9635 - val_accuracy: 0.3597 - val_loss: 3.5331 - val_top_5_accuracy: 0.6413\n",
      "Epoch 19/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 29ms/step - accuracy: 0.7812 - loss: 0.7119 - top_5_accuracy: 0.9695 - val_accuracy: 0.3643 - val_loss: 3.6189 - val_top_5_accuracy: 0.6405\n",
      "Epoch 20/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.7894 - loss: 0.6910 - top_5_accuracy: 0.9722 - val_accuracy: 0.3525 - val_loss: 3.7319 - val_top_5_accuracy: 0.6369\n",
      "Epoch 21/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 24ms/step - accuracy: 0.7988 - loss: 0.6525 - top_5_accuracy: 0.9758 - val_accuracy: 0.3649 - val_loss: 3.7843 - val_top_5_accuracy: 0.6390\n",
      "Epoch 22/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 22ms/step - accuracy: 0.8111 - loss: 0.6115 - top_5_accuracy: 0.9781 - val_accuracy: 0.3632 - val_loss: 3.8986 - val_top_5_accuracy: 0.6436\n",
      "Epoch 23/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 22ms/step - accuracy: 0.8083 - loss: 0.6229 - top_5_accuracy: 0.9771 - val_accuracy: 0.3561 - val_loss: 3.9053 - val_top_5_accuracy: 0.6424\n",
      "Epoch 24/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 22ms/step - accuracy: 0.8207 - loss: 0.5753 - top_5_accuracy: 0.9803 - val_accuracy: 0.3553 - val_loss: 4.0632 - val_top_5_accuracy: 0.6345\n",
      "Epoch 25/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 22ms/step - accuracy: 0.8218 - loss: 0.5741 - top_5_accuracy: 0.9811 - val_accuracy: 0.3579 - val_loss: 4.0565 - val_top_5_accuracy: 0.6381\n",
      "Epoch 26/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 22ms/step - accuracy: 0.8301 - loss: 0.5461 - top_5_accuracy: 0.9828 - val_accuracy: 0.3590 - val_loss: 4.0885 - val_top_5_accuracy: 0.6380\n",
      "Epoch 27/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 22ms/step - accuracy: 0.8366 - loss: 0.5288 - top_5_accuracy: 0.9840 - val_accuracy: 0.3532 - val_loss: 4.1213 - val_top_5_accuracy: 0.6317\n",
      "Epoch 28/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 22ms/step - accuracy: 0.8382 - loss: 0.5157 - top_5_accuracy: 0.9856 - val_accuracy: 0.3609 - val_loss: 4.2777 - val_top_5_accuracy: 0.6360\n",
      "Epoch 29/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 22ms/step - accuracy: 0.8448 - loss: 0.5032 - top_5_accuracy: 0.9858 - val_accuracy: 0.3553 - val_loss: 4.3570 - val_top_5_accuracy: 0.6362\n",
      "Epoch 30/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 22ms/step - accuracy: 0.8452 - loss: 0.4969 - top_5_accuracy: 0.9850 - val_accuracy: 0.3572 - val_loss: 4.4464 - val_top_5_accuracy: 0.6394\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train_new, # Training data\n",
    "    y_train_new, # Training labels\n",
    "    epochs=30, # Number of epochs -- use a larger number due to the dataset being larger \n",
    "    batch_size=32, # Number of samples per batch\n",
    "    validation_data=(x_test_new, y_test_new)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48615c26b99d2e9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 3b: Accuracy and other relevant metrics on the test set\n",
    "\n",
    "- Report the accuracy of your model on the test set.\n",
    "- While accuracy is a good metric, there are many other ways to numerically evaluate a model. Report at least one other metric, and explain what it measures and how it is calculated.\n",
    "\n",
    "- <b>Accuracy:</b> ______\n",
    "- <b>Other metric:</b> ______\n",
    "- <b>Reason for selection:</b> _____\n",
    "- <b>Value of metric:</b> ______\n",
    "- <b>Interpretation of metric value:</b> ______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f670665fda92fb0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T17:49:39.016880Z",
     "start_time": "2024-01-26T17:49:39.012100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.3584 - loss: 4.4786 - top_5_accuracy: 0.6393\n",
      "Loss:     4.45\n",
      "Accuracy: 35.72%\n",
      "Top-5 Accuracy: 63.94%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, top_k_accuracy = model.evaluate(x_test_new, y_test_new)\n",
    "\n",
    "print(f'Loss:     {loss:.2f}')\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "print(f'Top-5 Accuracy: {top_k_accuracy*100:.2f}%') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top k accuracy measures the probability predictions of all the classes and determines whether the correct classes appear within the ones with the top scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d2d836d4e8ce99",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 3c: Visualize the model's learning\n",
    "\n",
    "- Plot the training accuracy and validation accuracy with respect to epochs.\n",
    "- Select an image that the model correctly classified in the test set, and an image that the model incorrectly classified in the test set. Plot the images and report the model's classification probabilities for each.\n",
    "- Briefly discuss the results. What do the plots show? Do the results make sense? What do the classification probabilities indicate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c5b214475a496ca5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T17:50:59.733968Z",
     "start_time": "2024-01-26T17:50:59.730635Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 0 Axes>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a648758ebea0561d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 4: Model Enhancement (Complete or Incomplete)\n",
    "### Task 4a: Implementation of at least one advanced technique\n",
    "\n",
    "- Now it's time to improve your model. Implement at least one technique to improve your model's performance. You can use any of the techniques we have covered in class, or you can use a technique that we haven't covered. If you need inspiration, you can refer to the [Keras documentation](https://keras.io/).\n",
    "- Explain the technique you used and why you chose it.\n",
    "- If you used a technique that requires tuning, explain how you selected the values for the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3659ac83122567f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Teresa - School\\anaconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,300</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_7 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m51,300\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,250,852</span> (4.77 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,250,852\u001b[0m (4.77 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,250,852</span> (4.77 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,250,852\u001b[0m (4.77 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a CNN model \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "# Add convolution layers \n",
    "model2.add(Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model2.add(MaxPooling2D((2,2)))\n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "model2.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model2.add(MaxPooling2D((2,2)))\n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "# Add fully connected layers\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(512, activation='relu'))\n",
    "model2.add(Dense(100, activation='softmax'))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9467a483a1dd5d3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 4b: Evaluation of the enhanced model\n",
    "\n",
    "- Re-train your model using the same number of epochs as before.\n",
    "- Compare the accuracy and other selected metric on the test set to the results you obtained before.\n",
    "- As before, plot the training accuracy and validation accuracy with respect to epochs, and select an image that the model correctly classified in the test set, and an image that the model incorrectly classified in the test set. Plot the images and report the model's classification probabilities for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7c4701b36dc8fc55",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 25ms/step - accuracy: 0.0351 - loss: 4.4441 - top_5_accuracy: 0.1248 - val_accuracy: 0.1171 - val_loss: 3.9091 - val_top_5_accuracy: 0.3222\n",
      "Epoch 2/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 21ms/step - accuracy: 0.1332 - loss: 3.7954 - top_5_accuracy: 0.3451 - val_accuracy: 0.1632 - val_loss: 3.6175 - val_top_5_accuracy: 0.4141\n",
      "Epoch 3/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 26ms/step - accuracy: 0.1726 - loss: 3.5462 - top_5_accuracy: 0.4177 - val_accuracy: 0.1948 - val_loss: 3.4722 - val_top_5_accuracy: 0.4534\n",
      "Epoch 4/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.1975 - loss: 3.4159 - top_5_accuracy: 0.4517 - val_accuracy: 0.2180 - val_loss: 3.3331 - val_top_5_accuracy: 0.4822\n",
      "Epoch 5/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 26ms/step - accuracy: 0.2257 - loss: 3.2588 - top_5_accuracy: 0.4958 - val_accuracy: 0.2372 - val_loss: 3.2351 - val_top_5_accuracy: 0.5125\n",
      "Epoch 6/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 26ms/step - accuracy: 0.2440 - loss: 3.1567 - top_5_accuracy: 0.5229 - val_accuracy: 0.2509 - val_loss: 3.1552 - val_top_5_accuracy: 0.5368\n",
      "Epoch 7/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.2661 - loss: 3.0406 - top_5_accuracy: 0.5508 - val_accuracy: 0.2652 - val_loss: 3.0596 - val_top_5_accuracy: 0.5507\n",
      "Epoch 8/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 26ms/step - accuracy: 0.2802 - loss: 2.9532 - top_5_accuracy: 0.5708 - val_accuracy: 0.2802 - val_loss: 2.9998 - val_top_5_accuracy: 0.5683\n",
      "Epoch 9/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 26ms/step - accuracy: 0.3017 - loss: 2.8655 - top_5_accuracy: 0.5939 - val_accuracy: 0.2925 - val_loss: 2.9377 - val_top_5_accuracy: 0.5830\n",
      "Epoch 10/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 26ms/step - accuracy: 0.3099 - loss: 2.7960 - top_5_accuracy: 0.6112 - val_accuracy: 0.3041 - val_loss: 2.8817 - val_top_5_accuracy: 0.5890\n",
      "Epoch 11/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 27ms/step - accuracy: 0.3231 - loss: 2.7242 - top_5_accuracy: 0.6242 - val_accuracy: 0.3112 - val_loss: 2.8415 - val_top_5_accuracy: 0.6004\n",
      "Epoch 12/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.3403 - loss: 2.6506 - top_5_accuracy: 0.6435 - val_accuracy: 0.3209 - val_loss: 2.8052 - val_top_5_accuracy: 0.6098\n",
      "Epoch 13/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.3482 - loss: 2.6156 - top_5_accuracy: 0.6494 - val_accuracy: 0.3271 - val_loss: 2.7666 - val_top_5_accuracy: 0.6182\n",
      "Epoch 14/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.3655 - loss: 2.5374 - top_5_accuracy: 0.6656 - val_accuracy: 0.3360 - val_loss: 2.7157 - val_top_5_accuracy: 0.6232\n",
      "Epoch 15/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.3741 - loss: 2.4988 - top_5_accuracy: 0.6743 - val_accuracy: 0.3420 - val_loss: 2.7030 - val_top_5_accuracy: 0.6300\n",
      "Epoch 16/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.3854 - loss: 2.4370 - top_5_accuracy: 0.6882 - val_accuracy: 0.3459 - val_loss: 2.6704 - val_top_5_accuracy: 0.6324\n",
      "Epoch 17/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.3977 - loss: 2.3663 - top_5_accuracy: 0.6993 - val_accuracy: 0.3479 - val_loss: 2.6527 - val_top_5_accuracy: 0.6379\n",
      "Epoch 18/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 29ms/step - accuracy: 0.4041 - loss: 2.3472 - top_5_accuracy: 0.7053 - val_accuracy: 0.3551 - val_loss: 2.6214 - val_top_5_accuracy: 0.6453\n",
      "Epoch 19/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.4157 - loss: 2.2825 - top_5_accuracy: 0.7195 - val_accuracy: 0.3637 - val_loss: 2.5964 - val_top_5_accuracy: 0.6528\n",
      "Epoch 20/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.4317 - loss: 2.2388 - top_5_accuracy: 0.7311 - val_accuracy: 0.3695 - val_loss: 2.5694 - val_top_5_accuracy: 0.6568\n",
      "Epoch 21/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 29ms/step - accuracy: 0.4395 - loss: 2.1826 - top_5_accuracy: 0.7368 - val_accuracy: 0.3697 - val_loss: 2.5574 - val_top_5_accuracy: 0.6569\n",
      "Epoch 22/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.4508 - loss: 2.1306 - top_5_accuracy: 0.7478 - val_accuracy: 0.3771 - val_loss: 2.5451 - val_top_5_accuracy: 0.6600\n",
      "Epoch 23/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.4608 - loss: 2.0839 - top_5_accuracy: 0.7549 - val_accuracy: 0.3795 - val_loss: 2.5301 - val_top_5_accuracy: 0.6665\n",
      "Epoch 24/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.4679 - loss: 2.0543 - top_5_accuracy: 0.7617 - val_accuracy: 0.3795 - val_loss: 2.5390 - val_top_5_accuracy: 0.6636\n",
      "Epoch 25/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.4817 - loss: 1.9968 - top_5_accuracy: 0.7740 - val_accuracy: 0.3814 - val_loss: 2.5095 - val_top_5_accuracy: 0.6674\n",
      "Epoch 26/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 26ms/step - accuracy: 0.4900 - loss: 1.9516 - top_5_accuracy: 0.7822 - val_accuracy: 0.3867 - val_loss: 2.4973 - val_top_5_accuracy: 0.6733\n",
      "Epoch 27/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 26ms/step - accuracy: 0.4980 - loss: 1.9190 - top_5_accuracy: 0.7881 - val_accuracy: 0.3907 - val_loss: 2.4915 - val_top_5_accuracy: 0.6763\n",
      "Epoch 28/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5050 - loss: 1.8825 - top_5_accuracy: 0.7959 - val_accuracy: 0.3907 - val_loss: 2.4787 - val_top_5_accuracy: 0.6751\n",
      "Epoch 29/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 26ms/step - accuracy: 0.5174 - loss: 1.8242 - top_5_accuracy: 0.8067 - val_accuracy: 0.3927 - val_loss: 2.4842 - val_top_5_accuracy: 0.6768\n",
      "Epoch 30/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 26ms/step - accuracy: 0.5311 - loss: 1.7797 - top_5_accuracy: 0.8145 - val_accuracy: 0.3948 - val_loss: 2.4827 - val_top_5_accuracy: 0.6767\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.4007 - loss: 2.4637 - top_5_accuracy: 0.6805\n",
      "Loss:     2.48\n",
      "Accuracy: 39.48%\n",
      "Top-5 Accuracy: 67.67%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model2.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    ")\n",
    "\n",
    "history2 = model2.fit(\n",
    "    x_train_new, # Training data\n",
    "    y_train_new, # Training labels\n",
    "    epochs=30, # Number of epochs -- use a larger number due to the dataset being larger \n",
    "    batch_size=32, # Number of samples per batch\n",
    "    validation_data=(x_test_new, y_test_new)\n",
    ")\n",
    "\n",
    "loss, accuracy, top_k_accuracy = model2.evaluate(x_test_new, y_test_new)\n",
    "\n",
    "print(f'Loss:     {loss:.2f}')\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "print(f'Top-5 Accuracy: {top_k_accuracy*100:.2f}%') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadfc848700215e8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 4c: Discussion of the results\n",
    "\n",
    "- Briefly discuss the results. \n",
    "- Did the model's performance improve? \n",
    "- Why do you think this is?\n",
    "- Do you think there is room for further improvement? Why or why not?\n",
    "- What other techniques might you try in the future?\n",
    "- Your answer should be no more than 200 words.\n",
    "\n",
    "My model's performance improved after the enhancements were made. I think there can be further improvements made based on the current accuracy level. I think it improved due to increasing the number of neurons in the Dense layer, which helps the model to learn more complex patterns. In addition, I think adding a softmax activation in my output layer may have helped improved the model's performance by making it better at making decisions regarding classifying classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7415f68f",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "|Criteria|Complete|Incomplete|\n",
    "|----|----|----|\n",
    "|Task 1|The task has been completed successfully and there are no errors.|The task is still incomplete and there is at least one error.|\n",
    "|Task 2|The task has been completed successfully and there are no errors.|The task is still incomplete and there is at least one error.|\n",
    "|Task 3|The task has been completed successfully and there are no errors.|The task is still incomplete and there is at least one error.|\n",
    "|Task 4|The task has been completed successfully and there are no errors.|The task is still incomplete and there is at least one error.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c10bc",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "🚨 **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** 🚨 for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-1`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/deep_learning/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
