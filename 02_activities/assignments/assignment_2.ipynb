{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "197fbb2d",
   "metadata": {},
   "source": [
    "# Assignment 2 ‚Äì Zero-Shot Image Classification with Transformers\n",
    "\n",
    "In this assignment, you will apply a pre-trained vision‚Äìlanguage transformer (e.g. CLIP) to perform **zero-shot** classification on the Fashion-MNIST dataset‚Äîclassifying each image without any task-specific training. You will build on the concepts from Assignment 1 by comparing this ‚Äúoff-the-shelf‚Äù approach to the CNN you previously trained.\n",
    "\n",
    "You will:\n",
    "1. **Load** the Fashion-MNIST images using PyTorch instead of Keras.\n",
    "2. **Run a zero-shot baseline** with simple text prompts to set a performance reference.\n",
    "3. **Engineer improved prompts** and measure the resulting accuracy gains.\n",
    "4. **Visualise image embeddings** with UMAP to inspect class separability.\n",
    "5. **Conduct one mini-experiment** of your choice.\n",
    "6. **Summarise findings** and reflect on strengths and weaknesses of zero-shot transformers versus a trained CNN.\n",
    "\n",
    "# 1. Loading the Fashion-MNIST Dataset\n",
    "\n",
    "As in assignment 1, we'll load the Fashion-MNIST dataset, but this time using `torchvision.datasets` to ensure compatibility with the `transformers` library. We will also load our model and processor from the `transformers` library.\n",
    "\n",
    "The transformers library allows us to use pre-trained models like CLIP, which can perform zero-shot classification by leveraging the text prompts we provide. There are two key objects we will use: the `CLIPModel` for the model itself and the `CLIPProcessor` for preparing our images and text prompts.\n",
    "\n",
    "Since we are not actually training a model in this assignment, we will set the CLIP model to evaluation mode. If the model is designed to utilize features like dropout or batch normalization, setting it to evaluation mode ensures that these features behave correctly during inference (prediction). Setting the model to evaluaton mode also tells PyTorch that we don't have to compute gradients, which can save memory and speed up inference.\n",
    "\n",
    "In order to speed up processing, we will also move the model to an \"accelerator\" if available. This is typically a GPU, but modern MacBooks also have an \"Apple Silicon\" accelerator that can be used for inference, called MPS (Metal Performance Shaders). If you are using a MacBook with Apple Silicon, you can use the MPS device for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "026463ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (4.57.1)\n",
      "Requirement already satisfied: torchvision in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (0.17.2)\n",
      "Requirement already satisfied: torch in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: accelerate in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (1.11.0)\n",
      "Requirement already satisfied: filelock in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: sympy in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: psutil in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from accelerate) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment and run if required\n",
    "!pip install transformers torchvision torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4701b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewharris/micromamba/envs/tf-cpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-30 20:13:45.588723: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m clip_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip-vit-base-patch32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m clip_model     \u001b[38;5;241m=\u001b[39m \u001b[43mCLIPModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m clip_processor \u001b[38;5;241m=\u001b[39m CLIPProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(clip_model_name, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Set model to evaluation mode, as we are not training it\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/tf-cpu/lib/python3.10/site-packages/transformers/modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/micromamba/envs/tf-cpu/lib/python3.10/site-packages/transformers/modeling_utils.py:5048\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5039\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   5041\u001b[0m     (\n\u001b[1;32m   5042\u001b[0m         model,\n\u001b[1;32m   5043\u001b[0m         missing_keys,\n\u001b[1;32m   5044\u001b[0m         unexpected_keys,\n\u001b[1;32m   5045\u001b[0m         mismatched_keys,\n\u001b[1;32m   5046\u001b[0m         offload_index,\n\u001b[1;32m   5047\u001b[0m         error_msgs,\n\u001b[0;32m-> 5048\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5054\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5057\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5059\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5060\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5063\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5064\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   5065\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/micromamba/envs/tf-cpu/lib/python3.10/site-packages/transformers/modeling_utils.py:5316\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5313\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(state_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   5314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5315\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m-> 5316\u001b[0m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   5317\u001b[0m     )\n\u001b[1;32m   5319\u001b[0m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[1;32m   5320\u001b[0m prefix \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbase_model_prefix\n",
      "File \u001b[0;32m~/micromamba/envs/tf-cpu/lib/python3.10/site-packages/transformers/modeling_utils.py:508\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[0;32m--> 508\u001b[0m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/tf-cpu/lib/python3.10/site-packages/transformers/utils/import_utils.py:1647\u001b[0m, in \u001b[0;36mcheck_torch_load_is_safe\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_torch_load_is_safe\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1646\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.6\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1647\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1648\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1649\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1650\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen loading files with safetensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1652\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è∞ Cell finished in 133.7 seconds.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import torch\n",
    "\n",
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_model     = CLIPModel.from_pretrained(clip_model_name)\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_name, use_fast=False)\n",
    "\n",
    "# Set model to evaluation mode, as we are not training it\n",
    "clip_model.eval()\n",
    "\n",
    "# Check for accelerators\n",
    "device = \"cpu\" # Default to CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" # Use GPU if available\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "clip_model.to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14789e25",
   "metadata": {},
   "source": [
    "Now we are ready to load the testing set from Fashion-MNIST. We will use the `torchvision.datasets.FashionMNIST` class to load the dataset. We do not need to apply any transformations to the images, as the `CLIPProcessor` ensures any input images are in the format that the model is trained on.\n",
    "\n",
    "You should:\n",
    "\n",
    "- [ ] Use the `torchvision.datasets.FashionMNIST` class to load the *test* split of the dataset. Documentation is available [here](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html).\n",
    "- [ ] Create a PyTorch `DataLoader` to iterate over the dataset in batches. Use a batch size of 16 and set `shuffle=True` to randomise the order of the images. You will also need to supply the provided `collate_clip` function to the `DataLoader collate_fn` argument to ensure the images are processed correctly. Documentation for `DataLoader` is available [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\"\n",
    "]\n",
    "\n",
    "def collate_clip(batch):\n",
    "    imgs, labels = zip(*batch) # Unzip the batch into images and labels\n",
    "    proc = clip_processor(images=list(imgs),\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True) # Process images with CLIPProcessor\n",
    "    # Send pixel_values to GPU/CPU now; labels stay on CPU for metrics\n",
    "    return proc[\"pixel_values\"].to(device), torch.tensor(labels)\n",
    "\n",
    "test_dataset = # Complete\n",
    "test_loader = # Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed01b1c",
   "metadata": {},
   "source": [
    "If your code is correct, the following cell should show the first batch of images from the Fashion-MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d97e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the first batch of images from `test_loader`\n",
    "\n",
    "def show_batch(loader):\n",
    "    images, labels = next(iter(loader))\n",
    "    images = images.cpu()  # Move images to CPU for plotting\n",
    "    # Renormalize to [0, 1] for visualization\n",
    "    images = (images - images.min()) / (images.max() - images.min())\n",
    "    _, axes = plt.subplots(1, len(images), figsize=(15, 5))\n",
    "    for ax, img, label in zip(axes, images, labels):\n",
    "        ax.imshow(img.permute(1, 2, 0))\n",
    "        ax.set_title(CLASS_NAMES[label.item()])\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_batch(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1183726",
   "metadata": {},
   "source": [
    "We‚Äôre now ready to run our zero-shot classification baseline!\n",
    "\n",
    "# Brief Introduction to Zero-Shot Classification\n",
    "\n",
    "In Assignment 1, we followed the typical machine-learning pipeline: we trained a CNN on the Fashion-MNIST dataset, using labelled examples to update the model‚Äôs weights. While effective, that approach requires a curated, task-specific training set‚Äîa luxury you don‚Äôt always have in practice.\n",
    "\n",
    "Zero-shot classification flips the script.  A large vision‚Äìlanguage model (VLM) such as **CLIP** is first pre-trained on hundreds of millions of image‚Äìtext pairs scraped from the web.  Because it learns *joint* visual‚Äìtextual embeddings, the model can later solve new tasks simply by ‚Äúmeasuring‚Äù how similar an image is to a **text prompt** that describes each candidate class‚Äîwithout seeing a single task-labelled example.\n",
    "\n",
    "**How it works**  \n",
    "1. Feed an image through CLIP‚Äôs vision encoder ‚Üí **image feature**.  \n",
    "2. Feed a textual prompt (e.g. ‚Äúa photo of a sandal‚Äù) through CLIP‚Äôs text encoder ‚Üí **text feature**.  \n",
    "3. Compute cosine similarity between the image feature and every class‚Äôs text feature.  \n",
    "4. Pick the class whose prompt is most similar.\n",
    "\n",
    "For our first attempt, we‚Äôll use the bare class names as prompts, e.g.:\n",
    "\n",
    "- \"T-shirt/top\"\n",
    "- \"Trouser\"\n",
    "\n",
    "### You should:\n",
    "\n",
    "- [ ] Build embeddings: use the `get_text_embeddings` helper function to create text embeddings for the class names.\n",
    "- [ ] Run inference: use the `get_image_embeddings` helper function to create image embeddings.\n",
    "- [ ] Compute cosine similarity: complete and use the `get_cosine_similarity` helper function to compute the cosine similarity between the image and text embeddings.\n",
    "- [ ] Make predictions: use the `get_predictions` helper function to get the predicted class for each image in the batch.\n",
    "\n",
    "Note that for normalized vectors like the ones we are using, cosine similarity is equivalent to the dot product. This means we can use the handy formula `cosine_similarity = vector_a @ vector_b.T` to compute the similarity between the image and text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3acc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embeddings(class_names: list[str]) -> torch.Tensor:\n",
    "    \"\"\"    Get text embeddings for the given class names using CLIP.\n",
    "    Args:\n",
    "        class_names (list[str]): List of class names to encode.\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized text embeddings for the class names.\n",
    "    \"\"\"\n",
    "    tokenized = clip_processor(text=class_names,\n",
    "                               padding=True,\n",
    "                               return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_embeddings = clip_model.get_text_features(**tokenized)\n",
    "\n",
    "    text_feats = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return text_feats\n",
    "\n",
    "def get_image_embeddings(images: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"    Get image embeddings for the given images using CLIP.\n",
    "    Args:\n",
    "        images (torch.Tensor): Batch of images to encode.\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized image embeddings for the images.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        image_embeddings = clip_model.get_image_features(pixel_values=images)\n",
    "\n",
    "    image_feats = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return image_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16653654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_cosine_similarity(image_feats: torch.Tensor, text_feats: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between image features and text features.\n",
    "    Args:\n",
    "        image_feats (torch.Tensor): Image features of shape (N, D).\n",
    "        text_feats (torch.Tensor): Text features of shape (M, D).\n",
    "    Returns:\n",
    "        numpy.ndarray: Cosine similarity matrix of shape (N, M), where N is the number of images and M is the number of text prompts.\n",
    "    \"\"\"\n",
    "    image_feats = image_feats.cpu()  # Ensure image features are on CPU\n",
    "    text_feats = text_feats.cpu()    # Ensure text features are on CPU\n",
    "\n",
    "    # Compute cosine similarity, which is the dot product of normalized vectors\n",
    "    return # Complete\n",
    "\n",
    "def get_predictions(similarity: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get predictions based on cosine similarity scores.\n",
    "    Args:\n",
    "        similarity (numpy.ndarray): Cosine similarity matrix of shape (N, M), where N is the number of images and M is the number of text prompts.\n",
    "    Returns:\n",
    "        numpy.ndarray: Predicted class indices for each image, shape (N,).\n",
    "    \"\"\"\n",
    "    # Get the index of the maximum similarity for each image\n",
    "    return # Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d9e1de",
   "metadata": {},
   "source": [
    "With these functions complete, you are ready to run the zero-shot classification baseline. Complete the code to follow these steps:\n",
    "\n",
    "- [ ] Build text embeddings for the class names using the `get_text_embeddings` function (this only needs to be done once).\n",
    "- [ ] For each batch of images:\n",
    "    - [ ] Get image embeddings using the `get_image_embeddings` function.\n",
    "    - [ ] Compute cosine similarity between the image and text embeddings using the `get_cosine_similarity` function.\n",
    "    - [ ] Save the predictions so that we can build a confusion matrix later.\n",
    "- [ ] Report the accuracy of the predictions and the confusion matrix using the `accuracy_score` and `confusion_matrix` functions from `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4364acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "for pixel_values, labels in test_loader:\n",
    "\n",
    "# Report the accuracy of the predictions\n",
    "\n",
    "# Report the confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0857f4a8",
   "metadata": {},
   "source": [
    "Reflection: Consider the results. How does the performance of this zero-shot baseline compare to the CNN you trained in Assignment 1? What are the strengths and weaknesses of this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e16af",
   "metadata": {},
   "source": [
    "## Improving Zero-Shot Classification with Prompt Engineering\n",
    "\n",
    "In the previous section, we directly used the class names as text prompts for zero-shot classification. However, we can often improve performance by crafting more descriptive prompts that better capture the visual characteristics of each class. For example, instead of just \"T-shirt/top\", we could use \"a photo of a T-shirt\" or \"a photo of a top\". This additional context can help the model make more accurate predictions.\n",
    "\n",
    "In this section, we will experiment with more detailed prompts for each class to see if we can improve the zero-shot classification performance. You should:\n",
    "\n",
    "- [ ] Create a list of improved prompts for each class. For example, instead of just \"T-shirt/top\", you could use \"a photo of a T-shirt\" or \"a photo of a top\".\n",
    "- [ ] Use the `get_text_embeddings` function to create text embeddings for the improved prompts.\n",
    "- [ ] Run the zero-shot classification baseline again using the improved prompts and report the accuracy and confusion matrix.\n",
    "\n",
    "Note: Take advantage of the confusion matrix above. If two classes are often confused, consider how you might improve the prompts to help the model distinguish them better.\n",
    "\n",
    "The aim for this section is for you to improve the performance of the model. However, if you find that the performance does not improve significantly, you can still reflect on the process and consider how you might further refine the prompts with more effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf5cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28e88284",
   "metadata": {},
   "source": [
    "Reflection: How did your detailed prompts affect the zero-shot classification performance? Did you see a significant improvement compared to the baseline? What insights did you gain about the model's understanding of the classes? Do you think that with more effort you could further improve the performance? If so, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e817d7b4",
   "metadata": {},
   "source": [
    "## Visualizing Image Embeddings with UMAP\n",
    "\n",
    "To better understand how the model perceives the different classes, we can visualize the image embeddings using UMAP (Uniform Manifold Approximation and Projection). UMAP is a dimensionality reduction technique that helps us see how similar or dissimilar the embeddings are in a lower-dimensional space.\n",
    "\n",
    "By visualizing the embeddings, we can gain insights into how well the model can distinguish certain images, even without considering the text prompts. This can help us identify clusters of similar images and see if there are any overlaps between classes.\n",
    "\n",
    "You should:\n",
    "\n",
    "- [ ] Use the `get_image_embeddings` function to get the image embeddings for the entire test set.\n",
    "- [ ] Use UMAP to reduce the dimensionality of the image embeddings to 2D.\n",
    "- [ ] Plot the 2D embeddings, coloring each point by its true class label.\n",
    "\n",
    "You may need to install the `umap-learn` library if you haven't already. You can do this by running `pip install umap-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a20757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to install UMAP if you haven't already\n",
    "# !pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Collect image embeddings\n",
    "# ------------------------------------------------------------\n",
    "all_img_emb = []\n",
    "all_labels  = []\n",
    "\n",
    "for pixel_values, labels in test_loader:\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Fit UMAP\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Plot coloured by ground-truth label\n",
    "# ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132f943e",
   "metadata": {},
   "source": [
    "The UMAP embeddings allow us to see how separable or non-separable different classes are with our specific model. If two specific images are very similar, then they will be placed near each other on this graph. \n",
    "\n",
    "Reflection: Do you notice any challenges in distinguishing images based on this figure? Are there any types of clothing in the dataset which the model has no trouble distinguishing from the others?\n",
    "\n",
    "## Mini-Experiment\n",
    "\n",
    "In this section, you will conduct a mini-experiment of your choice to further explore the capabilities of zero-shot classification with transformers. This can be anything you'd like, but here are some ideas to get you started.\n",
    "\n",
    "### A. Alternative Model\n",
    "\n",
    "So far we have been utilizing OpenAI's CLIP model for zero-shot classification. However, there are many other vision‚Äìlanguage models available in the `transformers` library that you can experiment with. For example, there are larger CLIP models such as [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14), and open-source versions such as [laion/CLIP-ViT-B-32-laion2B-s34B-b79K](https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K). You can also search huggingface [here](https://huggingface.co/models?sort=trending&search=clip) to find other models that might be suitable for zero-shot classification.\n",
    "\n",
    "You can try using a different model to see if it improves the zero-shot classification performance. You should:\n",
    "- [ ] Load a different model and processor from the `transformers` library.\n",
    "- [ ] Run the zero-shot classification baseline with the new model and report the accuracy and confusion matrix.\n",
    "- [ ] Reflect on the performance of the new model compared to the original CLIP model\n",
    "  - How does the new model perform compared to the original CLIP model?\n",
    "  - Do you notice any differences in the types of errors made by the new model?\n",
    "\n",
    "### B. Multiple-Description Classification\n",
    "\n",
    "Another interesting experiment is to explore multiple-description classification. *This involves providing multiple text prompts for each class, allowing the model to choose the most relevant one. For example, instead of just \"T-shirt/top\", you could provide \"a photo of a T-shirt\", \"a photo of a top\", and \"a photo of a shirt\". This can help the model better understand the class and increases the likelihood of a correct prediction. You should:\n",
    "\n",
    "- [ ] Create a list of multiple prompts for each class.\n",
    "- [ ] Use the `get_text_embeddings` function to create text embeddings for the multiple prompts.\n",
    "- [ ] Run the zero-shot classification baseline again using the multiple prompts and report the accuracy and confusion matrix.\n",
    "- [ ] Consider the model to be correct if it guesses *any* of the prompts belonging to the correct class.\n",
    "\n",
    "### C. Top-K Classification\n",
    "\n",
    "In some classification tasks, it can be useful to consider if the right answer is among the top K (e.g. top 3) predictions. This can be particularly useful in cases where the model is uncertain or when there are multiple similar classes. You should:\n",
    "\n",
    "- [ ] Modify the `get_predictions` function to return the top K predictions for each image.\n",
    "- [ ] Modify the accuracy calculation to consider the model correct if the true class is among the top K predictions.\n",
    "- [ ] Report the accuracy and confusion matrix for the top K predictions. Report at least two different values of K (e.g. K=2 and K=4).\n",
    "\n",
    "### D. Other Ideas\n",
    "\n",
    "You are welcome to come up with your own mini-experiment! Explain your idea in the report and implement it. Did it work as you expected? What did you learn from it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af85f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "234eab38",
   "metadata": {},
   "source": [
    "### Short Report\n",
    "\n",
    "In this section, you will write a short report summarizing your findings from the mini-experiment. The report should include the following sections:\n",
    "\n",
    "- **Introduction**: Briefly describe the mini-experiment you conducted and its objectives.\n",
    "- **Methodology**: Explain the steps you took to conduct the experiment, including any modifications you made to the code or model.\n",
    "- **Results**: Present the results of your experiment.\n",
    "- **Discussion**: Reflect on the performance of the model and the implications of your findings. Consider the strengths and weaknesses of zero-shot transformers versus a trained CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745659f3",
   "metadata": {},
   "source": [
    "üö® **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** üö® for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `23:59 PM - 02/11/2025`\n",
    "* The branch name for your repo should be: `assignment-2`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_2.ipynb)\n",
    "    * The Lab 4 notebook (labs/lab_4.ipynb)\n",
    "    * The Lab 5 notebook (labs/lab_5.ipynb)\n",
    "    * The Lab 6 notebook (labs/lab_6.ipynb)\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/deep_learning/pull/<pr_id>`\n",
    "* Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-7-help-ml`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
