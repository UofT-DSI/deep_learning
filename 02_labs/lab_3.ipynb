{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "\n",
    "In this lab, we'll be using Keras to build a recommender system. We'll be using the MovieLens dataset, a common benchmark dataset for recommender systems. \n",
    "\n",
    "MovieLens is a web-based recommender system and virtual community that recommends movies for its users to watch, based on their film preferences using collaborative filtering of members' movie ratings and movie reviews. You can check out the website here: https://movielens.org/\n",
    "\n",
    "We will download a subset of the dataset containing 100k ratings. There are tens of millions of ratings in the full dataset, spanning hundreds of thousands of users and movies. The subset we'll be using is a good example to demonstrate the concepts in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "from pathlib import Path\n",
    "\n",
    "ML_100K_URL = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "ML_100K_FILENAME = Path(\"ml-100k.zip\")\n",
    "ML_100K_FOLDER = Path(\"ml-100k\")\n",
    "\n",
    "if not ML_100K_FOLDER.exists():\n",
    "    if not ML_100K_FILENAME.exists():\n",
    "        urlretrieve(ML_100K_URL, ML_100K_FILENAME.name)\n",
    "    with ZipFile(ML_100K_FILENAME.name) as zip:\n",
    "        zip.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of MovieLens is to enable models to predict the rating a user would give to a movie they have not yet watched. This is a classic example of a recommendation system. The dataset is huge, and contains many parts giving information about the movies, the users, and the ratings. To begin with, we will look at the ratings file. Each line in the ratings file (u.data) is formatted as:\n",
    "\n",
    "`user_id, item_id, rating, timestamp`\n",
    "\n",
    "Which tells us a single user's rating of a single movie.\n",
    "\n",
    "We will start by loading the ratings data into a pandas dataframe and then take a look at the first few rows. If you haven't used Pandas before, it's an extremely powerful library for dealing with tabular data. You can think of it as a Python version of Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_ratings = pd.read_csv(ML_100K_FOLDER / \"u.data\", sep='\\t',\n",
    "                          names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "raw_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second file we'll look at is the movie metadata. This file (u.item) contains information about each movie, including the title and release date. Each line in the file is formatted as:\n",
    "\n",
    "`movie_id | movie_title | release_date | video_release_date | IMDb_URL | unknown | Action | Adventure | Animation | Children's | Comedy | Crime | Documentary | Drama | Fantasy | Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi | Thriller | War | Western`\n",
    "\n",
    "As you can see, the genres are binary variables. As with one-hot encoding, a 1 indicates that the movie is of that genre, and a 0 indicates that it is not. We aren't going to work with the genre data in this lab, but it's easy to imagine that it could be useful in a real-world recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['item_id', 'title', 'release_date', 'video_release_date', 'imdb_url']\n",
    "items = pd.read_csv(ML_100K_FOLDER / \"u.item\", sep='|', names=columns_to_keep,\n",
    "                    encoding='latin-1', usecols=range(5))\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `release_date` column is a string. We can convert it to a `datetime` object using the `pd.to_datetime` function. This will make it easier to work with in the future (if we want to do things like check which date came first, for example).\n",
    " \n",
    "We can also extract the year from the date and store it in a separate column. This will make it easier to do things like plot the number of movies released each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['release_date'] = pd.to_datetime(items['release_date']) # Pandas makes this easy!\n",
    "items['release_year'] = items['release_date'].dt.year # For later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, it will be easier to work with the data if we merge our two dataframes into a single dataframe. We can do this using the `merge` method. We'll merge the `items` dataframe into the `raw_ratings` dataframe, using the `item_id` column as the key. This will add the movie title and release year to each rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings = pd.merge(items, raw_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "It's always important to understand the data you've collected. Thankfully, Pandas continues to make this easy for us. Using the `describe` method, we can get a quick statistical summary of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a bit more pandas magic to compute the popularity of each movie (number of ratings). We will use the `groupby` method to group the dataframe by the `item_id` column and then use the `size` method to compute the number of ratings for each movie. We will use the `reset_index` method to convert the resulting Series into a dataframe with an `item_id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity = all_ratings.groupby('item_id').size().reset_index(name='popularity')\n",
    "items = pd.merge(popularity, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['popularity'].plot.hist(bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(items['popularity'] == 1).sum() # Number of movies with only one rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.nlargest(10, 'popularity')['title'] # Get the 10 most popular movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings = pd.merge(popularity, all_ratings)\n",
    "all_ratings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexed_items = items.set_index('item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Quick Exercise**:\n",
    "\n",
    "As we have seen, the `groupby` method is a powerful tool to quickly compute statistics on the data. Use it to compute the average rating for each movie.\n",
    "\n",
    "**Hint**: you can use the `mean` method after the `groupby` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the enriched data in a train / test split to make it possible to do predictive modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ratings_train, ratings_test = train_test_split(\n",
    "    all_ratings, test_size=0.2, random_state=0)\n",
    "\n",
    "user_id_train = np.array(ratings_train['user_id'])\n",
    "item_id_train = np.array(ratings_train['item_id'])\n",
    "rating_train = np.array(ratings_train['rating'])\n",
    "\n",
    "user_id_test = np.array(ratings_test['user_id'])\n",
    "item_id_test = np.array(ratings_test['item_id'])\n",
    "rating_test = np.array(ratings_test['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit feedback: supervised ratings prediction\n",
    "\n",
    "Now let's begin to do some recommendation! We will build a model that takes a user and a movie as input and outputs a predicted rating. We will be taking advantage of embeddings to represent users and movies. That means that each movie and user will have an abstract representation in a continuous vector space. The model will learn these representations based on the ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive ratings  as a regression problem\n",
    "\n",
    "The following code implements the following architecture:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/UofT-DSI/deep_learning/main/notebooks/images/rec_archi_1.svg\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Dot\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sample we input the integer identifiers\n",
    "# of a single user and a single item\n",
    "class RegressionModel(Model):\n",
    "    def __init__(self, embedding_size, max_user_id, max_item_id):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_user_id + 1,\n",
    "                                        input_length=1,\n",
    "                                        name='user_embedding')\n",
    "        self.item_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_item_id + 1,\n",
    "                                        input_length=1,\n",
    "                                        name='item_embedding')\n",
    "        \n",
    "        # The following two layers don't have parameters.\n",
    "        self.flatten = Flatten()\n",
    "        self.dot = Dot(axes=1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        user_inputs = inputs[0]\n",
    "        item_inputs = inputs[1]\n",
    "        \n",
    "        user_vecs = self.flatten(self.user_embedding(user_inputs))\n",
    "        item_vecs = self.flatten(self.item_embedding(item_inputs))\n",
    "        \n",
    "        y = self.dot([user_vecs, item_vecs])\n",
    "        return y\n",
    "\n",
    "\n",
    "model = RegressionModel(embedding_size=64, max_user_id=all_ratings['user_id'].max(), max_item_id=all_ratings['item_id'].max())\n",
    "model.compile(optimizer=\"adam\", loss='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring runs\n",
    "\n",
    "When training a model with Keras, we get a `history` object back that contains lots of information about the training run. We can use this to plot the training and validation loss to see how the model has improved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training the model\n",
    "history = model.fit([user_id_train, item_id_train], rating_train,\n",
    "                    batch_size=64, epochs=10, validation_split=0.1,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.ylim(0, 2)\n",
    "plt.legend(loc='best')\n",
    "plt.title('Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "\n",
    "- Does it look like our model has overfit? Why or why not?\n",
    "- Suggest something we could do to prevent overfitting.\n",
    "\n",
    "\n",
    "Now that the model is trained, let's check out the quality of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(y_true, y_pred):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.xlim(-1, 6)\n",
    "    plt.xlabel(\"True rating\")\n",
    "    plt.ylim(-1, 6)\n",
    "    plt.ylabel(\"Predicted rating\")\n",
    "    plt.scatter(y_true, y_pred, s=60, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "test_preds = model.predict([user_id_test, item_id_test])\n",
    "print(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\n",
    "print(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))\n",
    "plot_predictions(rating_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This graph shows us the range of predicted ratings our model gives, organized by the true rating. We can see that generally, the higher the true rating the higher the predicted rating, although there is quite a range of predictions for each instance. That's okay - our model is very simple, and human preferences are very complex!\n",
    "\n",
    "Taking a look at the Mean Absolute Error, hopefully you got something around 0.75. This means that, on average, our predicted ratings are about 0.75 stars off from the true ratings. This is a pretty good result for a first attempt. We could probably do better with a more complex model, but we'll leave that for another time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Embeddings\n",
    "\n",
    "Our model was built with two embedding layers. These layers have learned a representation of both the users and the movies in our dataset. We can extract these representations and use them to find similar movies or users. We can also do interesting exploratory analysis, like finding the most popular movies among our users, or finding the users that are most interested in a given movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights and shape\n",
    "weights = model.get_weights()\n",
    "[w.shape for w in weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = weights[0]\n",
    "item_embeddings = weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = 181\n",
    "print(f\"Title for item_id={item_id}: {indexed_items['title'][item_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Embedding vector for item_id={item_id}\")\n",
    "print(item_embeddings[item_id])\n",
    "print(\"shape:\", item_embeddings[item_id].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As we discussed in lecture, our embeddings are not directly interpretable - we can't look at, say, a value of 0.297 in the embedding vector and say \"this means that the movie is a drama\". As an aside, there is a field of research dedicated to making _interpretable_ embeddings, but it's not something we'll cover in this course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding our most similar items\n",
    "\n",
    "Now we can have some fun, investigating the embeddings we've learned. We can start by finding the most similar items to a given item. We can do this by computing the cosine similarity between the item's embedding and the embedding of every other item. We can use the `cosine_similarity` function from `sklearn` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine(a, b):\n",
    "    # Reshape to the shape our function expects\n",
    "    a = a.reshape(1, -1)\n",
    "    b = b.reshape(1, -1)\n",
    "    return cosine_similarity(a, b)[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similarity(item_a, item_b, item_embeddings, titles):\n",
    "    print(titles[item_a])\n",
    "    print(titles[item_b])\n",
    "    similarity = cosine(item_embeddings[item_a],\n",
    "                        item_embeddings[item_b])\n",
    "    print(f\"Cosine similarity: {similarity:.3}\")\n",
    "    \n",
    "print_similarity(50, 181, item_embeddings, indexed_items[\"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It makes sense that the original Star Wars, and its later sequel Return of the Jedi have a high similarity. Let's try some other examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_similarity(181, 288, item_embeddings, indexed_items[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_similarity(181, 1, item_embeddings, indexed_items[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_similarity(181, 181, item_embeddings, indexed_items[\"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "*Quick Exercise*:\n",
    "\n",
    "- Find some other films and compare their similarity. Do the results make sense to you? Can you find a pair of films that are very _dissimilar_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code to help you search for a movie title\n",
    "partial_title = \"Jedi\"\n",
    "indexed_items[indexed_items['title'].str.contains(partial_title)]\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Sometimes, even without knowing anything about a user, we can recommend films by asking them about a film that they do like. The code below compares the similarity of a given film to all others, and returns the most similar films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(item_id, item_embeddings, titles,\n",
    "                 top_n=30):\n",
    "    # Compute the cosine similarity between the item and all other items\n",
    "    sims = cosine_similarity(item_embeddings[item_id].reshape(1, -1),\n",
    "                             item_embeddings).ravel()\n",
    "    \n",
    "    # [::-1] makes it possible to reverse the order of a numpy\n",
    "    # array, this is required because most similar items have\n",
    "    # a larger cosine similarity value\n",
    "    sorted_indexes = np.argsort(sims)[::-1]\n",
    "    idxs = sorted_indexes[0:top_n]\n",
    "    return list(zip(idxs, titles[idxs], sims[idxs]))\n",
    "\n",
    "# Find the most similar films to \"Star Wars\"\n",
    "most_similar(50, item_embeddings, indexed_items[\"title\"], top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most similar films to \"Star Trek VI: The Undiscovered Country\"\n",
    "most_similar(227, item_embeddings, indexed_items[\"title\"], top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarities do not always make sense: the number of ratings is low and the embedding  does not automatically capture semantic relationships in that context. Better representations arise with higher number of ratings, and less overfitting  in models or maybe better loss function, such as those based on implicit feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing embeddings using TSNE\n",
    "\n",
    "The [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) algorithm enables us to visualize high dimensional vectors in a 2D space by preserving local neighborhoods. We can use it to get a 2D visualization of the item embeddings and see if similar items are close in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "item_tsne = TSNE(learning_rate=\"auto\", init=\"pca\", perplexity=30).fit_transform(item_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(item_tsne[:, 0], item_tsne[:, 1]);\n",
    "plt.xticks(()); plt.yticks(());\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "tsne_df = pd.DataFrame(item_tsne, columns=[\"tsne_1\", \"tsne_2\"])\n",
    "tsne_df[\"item_id\"] = np.arange(item_tsne.shape[0])\n",
    "tsne_df = tsne_df.merge(items.reset_index())\n",
    "\n",
    "px.scatter(tsne_df, x=\"tsne_1\", y=\"tsne_2\",\n",
    "           color=\"popularity\",\n",
    "           hover_data=[\"item_id\", \"title\", \"popularity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    " - Add another layer to the neural network and retrain, compare train/test error.\n",
    " - Try adding more dropout and change layer sizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A recommendation function for a given user\n",
    "\n",
    "Once the model is trained, the system can be used to recommend a few items for a user that they haven't seen before. The following code does that.\n",
    "\n",
    "- we use the `model.predict` to compute the ratings a user would have given to all items\n",
    "- we build a function that sorts these items and excludes those the user has already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def recommend(user_id, top_n=10):\n",
    "    item_ids = range(1, items['item_id'].max())\n",
    "    seen_mask = all_ratings[\"user_id\"] == user_id\n",
    "    seen_movies = set(all_ratings[seen_mask][\"item_id\"])\n",
    "    item_ids = list(filter(lambda x: x not in seen_movies, item_ids))\n",
    "\n",
    "    user = np.zeros_like(item_ids)\n",
    "    user[:len(item_ids)] = user_id\n",
    "    items_ = np.array(item_ids)\n",
    "    ratings = model.predict([user, items_]).flatten()\n",
    "    top_items = ratings.argsort()[-top_n:][::-1]\n",
    "    return [(indexed_items.loc[item_id][\"title\"], ratings[item_id]) for item_id in top_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, pred_rating in recommend(5):\n",
    "    print(\"    %0.1f: %s\" % (pred_rating, title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercises\n",
    "\n",
    "- Try modifying our neural network to improve recommendation. You could try adding more layers, or using a different loss function. \n",
    "- Your goal is to improve the Mean Absolute Error on the test set. Show the results of your best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extend and improve the model below\n",
    "class RegressionModel(Model):\n",
    "    def __init__(self, embedding_size, max_user_id, max_item_id):\n",
    "        super().__init__()\n",
    "\n",
    "        self.user_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_user_id + 1,\n",
    "                                        input_length=1,\n",
    "                                        name='user_embedding')\n",
    "        self.item_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_item_id + 1,\n",
    "                                        input_length=1,\n",
    "                                        name='item_embedding')\n",
    "\n",
    "        # The following two layers don't have parameters.\n",
    "        self.flatten = Flatten()\n",
    "        self.dot = Dot(axes=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_inputs = inputs[0]\n",
    "        item_inputs = inputs[1]\n",
    "\n",
    "        user_vecs = self.flatten(self.user_embedding(user_inputs))\n",
    "        item_vecs = self.flatten(self.item_embedding(item_inputs))\n",
    "\n",
    "        y = self.dot([user_vecs, item_vecs])\n",
    "        return y\n",
    "\n",
    "\n",
    "model = RegressionModel(embedding_size=64, max_user_id=all_ratings['user_id'].max(), max_item_id=all_ratings['item_id'].max())\n",
    "model.compile(optimizer=\"adam\", loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "history = model.fit([user_id_train, item_id_train], rating_train,\n",
    "                    batch_size=64, epochs=10, validation_split=0.1,\n",
    "                    shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
