{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification using Neural Networks\n",
    "\n",
    "The goal of this notebook is to learn to use Neural Networks for text classification.\n",
    "\n",
    "In this notebook, we will:\n",
    "- Train a shallow model with learning embeddings\n",
    "- Download pre-trained embeddings from Glove\n",
    "- Use these pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The BBC topic classification dataset\n",
    "\n",
    "The BBC provides some benchmark topic classification datasets in English at: http://mlg.ucd.ie/datasets/bbc.html.\n",
    "\n",
    "The raw text (encoded with the latin-1 character encoding) of the news can be downloaded as a ZIP archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip to bbc-fulltext.zip...\n",
      "Extracting contents of bbc-fulltext.zip...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import zipfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "BBC_DATASET_URL = \"http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip\"\n",
    "zip_filename = BBC_DATASET_URL.rsplit('/', 1)[1]\n",
    "BBC_DATASET_FOLDER = 'bbc'\n",
    "if not op.exists(zip_filename):\n",
    "    print(\"Downloading %s to %s...\" % (BBC_DATASET_URL, zip_filename))\n",
    "    urlretrieve(BBC_DATASET_URL, zip_filename)\n",
    "\n",
    "if not op.exists(BBC_DATASET_FOLDER):\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as f:\n",
    "        print(\"Extracting contents of %s...\" % zip_filename)\n",
    "        f.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the five folders contains text files from one of the five topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['business', 'entertainment', 'politics', 'sport', 'tech']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_names = sorted(folder for folder in os.listdir(BBC_DATASET_FOLDER)\n",
    "                      if op.isdir(op.join(BBC_DATASET_FOLDER, folder)))\n",
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ad sales boost Time Warner profit\n",
      "\n",
      "Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.\n",
      "\n",
      "The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\n",
      "\n",
      "Time ...\n"
     ]
    }
   ],
   "source": [
    "# Example of a file in the \"business\" category\n",
    "with open(op.join(BBC_DATASET_FOLDER, 'business', '001.txt'), 'rb') as f:\n",
    "    print(f.read().decode('latin-1')[:500] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomly partition the text files in a training and test set while recording the target category of each file as an integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = []\n",
    "filenames = []\n",
    "for target_id, target_name in enumerate(target_names):\n",
    "    class_path = op.join(BBC_DATASET_FOLDER, target_name) # e.g. 'bbc/business'\n",
    "    for filename in sorted(os.listdir(class_path)):\n",
    "        filenames.append(op.join(class_path, filename))\n",
    "        target.append(target_id)\n",
    "\n",
    "target = np.asarray(target, dtype=np.int32)\n",
    "target_train, target_test, filenames_train, filenames_test = train_test_split(\n",
    "    target, filenames, test_size=200, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What we now have is pairs of target labels (which category the document belongs to) and filenames (where the document is stored on disk):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 3, 4, 3], dtype=int32),\n",
       " ['bbc/business/475.txt',\n",
       "  'bbc/entertainment/152.txt',\n",
       "  'bbc/sport/127.txt',\n",
       "  'bbc/tech/095.txt',\n",
       "  'bbc/sport/464.txt'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train[:5], filenames_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 4.582 MB\n"
     ]
    }
   ],
   "source": [
    "size_in_bytes = sum([len(open(fn, 'rb').read()) for fn in filenames_train])\n",
    "print(\"Training set size: %0.3f MB\" % (size_in_bytes / 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is small so we can load everything into memory right now (which simplifies our code later). If we had substantially more data, we would need to use a `tf.data.Dataset` to stream it from disk in batches during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = [open(fn, 'rb').read().decode('latin-1') for fn in filenames_train]\n",
    "texts_test = [open(fn, 'rb').read().decode('latin-1') for fn in filenames_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first baseline model\n",
    "\n",
    "For simple topic classification problems, one should always try a simple method first. Let's try using a `CountVectorizer` followed by `LogisticRegression` as a baseline. What this will do is:\n",
    "\n",
    "- Convert the text documents to a matrix of token counts (each row is a document, each column is a word, each cell is the count of the word in the document)\n",
    "- Train a logistic regression model on this matrix\n",
    "\n",
    "It's a very efficient method and should give us a strong baseline to compare our deep learning method against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of the first document:\n",
      "Watchdog probes Vivendi bond sale\n",
      "\n",
      "French stock market regulator AMF has filed complaints against me...\n",
      "----------\n",
      "Sampling of vocabulary counts in the document:\n",
      "bank 2\n",
      "bankruptcy 0\n",
      "banks 0\n",
      "barcelona 0\n",
      "based 0\n",
      "basic 0\n",
      "basis 1\n",
      "bath 0\n",
      "battle 0\n",
      "bbc 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Understanding what the CountVectorizer does\n",
    "vectorizer = CountVectorizer(max_features=2000) # only keep the 2000 most frequent words\n",
    "X_train = vectorizer.fit_transform(texts_train)\n",
    "\n",
    "# Compare the content of the first document with the vocabulary\n",
    "print(\"Start of the first document:\")\n",
    "print(texts_train[0][0:100] + '...')\n",
    "print('----------')\n",
    "print(\"Sampling of vocabulary counts in the document:\")\n",
    "for word, count in zip(vectorizer.get_feature_names_out()[200:210], X_train.toarray()[0][200:210]):\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "text_classifier = make_pipeline(\n",
    "    CountVectorizer(max_features=2000),\n",
    "    LogisticRegression(max_iter=1000),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.8 s, sys: 1.02 s, total: 13.8 s\n",
      "Wall time: 33.5 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = text_classifier.fit(texts_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's check the accuracy of this baseline model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier.score(texts_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 95 percent testing accuracy on a very simple baseline. It's quite unlikely that we can significantly beat that baseline with a more complex deep learning based model. This is simply not a complex task - we wouldn't expect to see this level of performance from a simple model on a real-world text classification problem. Let's move on, and see how well we can do with a simple neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text for the (supervised) CBOW model\n",
    "\n",
    "We will implement a simple classification model in Keras. Raw text requires (sometimes a lot of) preprocessing.\n",
    "\n",
    "The following cells uses Keras to preprocess text:\n",
    "- using a tokenizer. This converts the texts into sequences of indices representing the `20000` most frequent words\n",
    "- sequences have different lengths, so we pad them (add 0s at the end until the sequence is of length `1000`). For example, if we were padding to three words, and we had a sequence of just \"dog\", we would pad it to \"[\\<dog token\\>,0,0]\".\n",
    "- we convert the output classes as 1-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 21:14:35.035173: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30995 unique tokens.\n",
      "Example of word_index: [('the', 1), ('to', 2), ('of', 3), ('and', 4), ('a', 5)]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, char_level=False)\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "sequences = tokenizer.texts_to_sequences(texts_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print(f'Example of word_index: {list(word_index.items())[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenized sequences are converted to list of token ids (with an integer code). We can convert them back to text to see what they now look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = dict((i, w) for w, i in tokenizer.word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:          ['Watchdog', 'probes', 'Vivendi', 'bond']\n",
      "Tokenized text:         [1857, 9454, 5251, 1973]\n",
      "Converted back to text: ['watchdog', 'probes', 'vivendi', 'bond']\n"
     ]
    }
   ],
   "source": [
    "print(f'Original text:          {texts_train[0].split(\" \")[0:4]}')\n",
    "print(f'Tokenized text:         {sequences[0][0:4]}')\n",
    "print(f'Converted back to text: {[index_to_word.get(i, \"UNK\") for i in sequences[0][0:4]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at the tokenized sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average length: 382.6\n",
      "max length: 4355\n"
     ]
    }
   ],
   "source": [
    "seq_lens = [len(s) for s in sequences]\n",
    "print(\"average length: %0.1f\" % np.mean(seq_lens))\n",
    "print(\"max length: %d\" % max(seq_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(seq_lens, bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while we do have sequences up to 4355 words long, the vast majority of sequences are less than 1000 words long. We can use this information to truncate or pad all the sequences to 1000 symbols to build the training set. This will simplify our model and speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (2025, 1000)\n",
      "Shape of data test tensor: (200, 1000)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "# Make all sequences exactly 1000 words long\n",
    "x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', x_train.shape)\n",
    "print('Shape of data test tensor:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (2025, 5)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(target_train)\n",
    "print('Shape of label tensor:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple supervised CBOW model in Keras\n",
    "\n",
    "The following computes a very simple model, as described in [fastText](https://github.com/facebookresearch/fastText):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/UofT-DSI/deep_learning/58b409e83f4294b293d6f763abf889d0a0fa1ddf/03_instructional_team/markdown_slides/images/svg_files/06_fasttext.svg\" style=\"width: 600px;\" />\n",
    "\n",
    "- Build an embedding layer mapping each word to a vector representation\n",
    "- Compute the vector representation of all words in each sequence and average them\n",
    "- Add a dense layer to output 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "N_CLASSES = len(target_names) \n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(MAX_NB_WORDS, EMBEDDING_DIM, trainable=True), # Just like we've seen in previous labs\n",
    "    GlobalAveragePooling1D(), # This layer averages the embeddings of all words in the sequence\n",
    "    Dense(N_CLASSES, activation='softmax') # This layer outputs a probability distribution over the 5 classes\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=Adam(learning_rate=0.01),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 213ms/step - accuracy: 0.3348 - loss: 1.4824 - val_accuracy: 0.4187 - val_loss: 1.3687\n",
      "Epoch 2/10\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 140ms/step - accuracy: 0.6037 - loss: 1.0593 - val_accuracy: 0.6847 - val_loss: 0.8372\n",
      "Epoch 3/10\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 145ms/step - accuracy: 0.7881 - loss: 0.6621 - val_accuracy: 0.7586 - val_loss: 0.6670\n",
      "Epoch 4/10\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 142ms/step - accuracy: 0.8595 - loss: 0.4263 - val_accuracy: 0.9310 - val_loss: 0.3117\n",
      "Epoch 5/10\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 105ms/step - accuracy: 0.9506 - loss: 0.2188 - val_accuracy: 0.9310 - val_loss: 0.2214\n",
      "Epoch 6/10\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 134ms/step - accuracy: 0.8985 - loss: 0.2992 - val_accuracy: 0.8522 - val_loss: 0.3920\n",
      "Epoch 7/10\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 121ms/step - accuracy: 0.9029 - loss: 0.2757 - val_accuracy: 0.9409 - val_loss: 0.1853\n",
      "Epoch 8/10\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 154ms/step - accuracy: 0.9868 - loss: 0.0785 - val_accuracy: 0.9803 - val_loss: 0.0961\n",
      "Epoch 9/10\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 127ms/step - accuracy: 0.9890 - loss: 0.0633 - val_accuracy: 0.9557 - val_loss: 0.1361\n",
      "Epoch 10/10\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 99ms/step - accuracy: 0.9929 - loss: 0.0463 - val_accuracy: 0.9606 - val_loss: 0.1279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13905ac80>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_split=0.1,\n",
    "          epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "- Evaluate the model on the test set\n",
    "- Identify an example of a mis-classified document and display the text of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building more complex models\n",
    "\n",
    "**Exercise**\n",
    "- Copy the previous model, and add more complexity to it. You can try adding more layers, or using a different type of layer (e.g. LSTM, Conv1D, etc.)\n",
    "- Some examples of what you could do:\n",
    "    - Add a LSTM layer before the dense layer ([LSTM documentation](https://keras.io/layers/recurrent/#lstm))\n",
    "    - Add a Conv1D layer after the embedding layer ([Conv1D documentation](https://keras.io/layers/convolutional/#conv1d))\n",
    "    - Add more dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Embedding, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential([\n",
    "    # Your code here\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sequential model sequential_1 cannot be built because it has no layers. Call `model.add(layer)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m output_test \u001b[38;5;241m=\u001b[39m model(x_test)\n\u001b[1;32m      5\u001b[0m test_casses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(output_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/DSI_participant/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/DSI_participant/lib/python3.10/site-packages/keras/src/models/sequential.py:172\u001b[0m, in \u001b[0;36mSequential.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers:\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequential model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be built because it has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno layers. Call `model.add(layer)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m     )\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m], InputLayer):\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbatch_shape \u001b[38;5;241m!=\u001b[39m input_shape:\n",
      "\u001b[0;31mValueError\u001b[0m: Sequential model sequential_1 cannot be built because it has no layers. Call `model.add(layer)`."
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_split=0.1,\n",
    "          epochs=15, batch_size=32)\n",
    "\n",
    "output_test = model(x_test)\n",
    "test_casses = np.argmax(output_test, axis=-1)\n",
    "print(\"Test accuracy:\", np.mean(test_casses == target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pre-trained embeddings\n",
    "\n",
    "The file `glove100K.100d.txt` is an extract of [Glove](http://nlp.stanford.edu/projects/glove/) Vectors, that were trained on English Wikipedia and the Gigaword 5 corpus. They differ from word2vec in the way the vectors are trained, but the idea is the same: each word is represented as a vector of `100` numbers.\n",
    "\n",
    "We extracted the `100 000` most frequent words for you, and the code below downloads them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get pretrained Glove Word2Vec\n",
    "URL_REPRESENTATIONS = \"https://github.com/m2dsupsdlclass/lectures-labs/releases/download/0.3/glove100k.100d.zip\"\n",
    "ZIP_REPRESENTATIONS = \"glove100k.100d.zip\"\n",
    "FILE_REPRESENTATIONS = \"glove100K.100d.txt\"\n",
    "\n",
    "if not op.exists(ZIP_REPRESENTATIONS):\n",
    "    print('Downloading from %s to %s...' % (URL_REPRESENTATIONS, ZIP_REPRESENTATIONS))\n",
    "    urlretrieve(URL_REPRESENTATIONS, './' + ZIP_REPRESENTATIONS)\n",
    "\n",
    "if not op.exists(FILE_REPRESENTATIONS):\n",
    "    print(\"extracting %s...\" % ZIP_REPRESENTATIONS)\n",
    "    myzip = zipfile.ZipFile(ZIP_REPRESENTATIONS)\n",
    "    myzip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "embeddings_vectors = []\n",
    "with open('glove100K.100d.txt', 'rb') as f:\n",
    "    word_idx = 0\n",
    "    for line in f:\n",
    "        values = line.decode('utf-8').split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = word_idx\n",
    "        embeddings_vectors.append(vector)\n",
    "        word_idx = word_idx + 1\n",
    "\n",
    "inv_index = {v: k for k, v in embeddings_index.items()}\n",
    "print(\"found %d different words in the file\" % word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all embeddings in a large numpy array\n",
    "glove_embeddings = np.vstack(embeddings_vectors)\n",
    "glove_norms = np.linalg.norm(glove_embeddings, axis=-1, keepdims=True)\n",
    "glove_embeddings_normed = glove_embeddings / glove_norms\n",
    "print(glove_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(word):\n",
    "    idx = embeddings_index.get(word)\n",
    "    if idx is None:\n",
    "        return None\n",
    "    else:\n",
    "        return glove_embeddings[idx]\n",
    "\n",
    "    \n",
    "def get_normed_emb(word):\n",
    "    idx = embeddings_index.get(word)\n",
    "    if idx is None:\n",
    "        return None\n",
    "    else:\n",
    "        return glove_embeddings_normed[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_emb(\"computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding most similar words\n",
    "\n",
    "Here we define a function to find the most similar words to a given word. The similarity is computed using the cosine similarity between the word embeddings. It can also accept multiple words, and it will take the average of the embeddings of the words to find the most similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(words, topn=10):\n",
    "    query_emb = 0\n",
    "    # If we have a list of words instead of one word\n",
    "    # (bonus question)\n",
    "    if type(words) == list:\n",
    "        for word in words:\n",
    "            query_emb += get_emb(word)\n",
    "    else:\n",
    "        query_emb = get_emb(words)\n",
    "\n",
    "    query_emb = query_emb / np.linalg.norm(query_emb)\n",
    "\n",
    "    # Large numpy vector with all cosine similarities\n",
    "    # between emb and all other words\n",
    "    cosines = np.dot(glove_embeddings_normed, query_emb)\n",
    "\n",
    "    # topn most similar indexes corresponding to cosines\n",
    "    idxs = np.argsort(cosines)[::-1][:topn]\n",
    "\n",
    "    # pretty return with word and similarity\n",
    "    return [(inv_index[idx], cosines[idx]) for idx in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(\"nvidia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bonus: sum of two word embeddings\n",
    "most_similar([\"toronto\", \"leaf\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Displaying vectors with  t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "word_emb_tsne = TSNE(perplexity=30).fit_transform(glove_embeddings_normed[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(40, 40))\n",
    "axis = plt.gca()\n",
    "np.set_printoptions(suppress=True)\n",
    "plt.scatter(word_emb_tsne[:, 0], word_emb_tsne[:, 1], marker=\".\", s=1)\n",
    "\n",
    "for idx in range(1000):\n",
    "    plt.annotate(inv_index[idx],\n",
    "                 xy=(word_emb_tsne[idx, 0], word_emb_tsne[idx, 1]),\n",
    "                 xytext=(0, 0), textcoords='offset points')\n",
    "plt.savefig(\"tsne.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre-trained embeddings in our model\n",
    "\n",
    "We want to use these pre-trained embeddings for transfer learning. This process is rather similar than transfer learning in image recognition: the features learnt on words might help us bootstrap the learning process, and increase performance if we don't have enough training data.\n",
    "\n",
    "- We initialize embedding matrix from the model with Glove embeddings:\n",
    " - take all unique words from our BBC news dataset to build a vocabulary (`MAX_NB_WORDS = 20000`), and look up their Glove embedding \n",
    " - place the Glove embedding at the corresponding index in the matrix\n",
    " - if the word is not in the Glove vocabulary, we only place zeros in the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# prepare embedding matrix\n",
    "nb_words_in_matrix = 0\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = get_emb(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector # Place the Glove embedding at the corresponding index in the matrix\n",
    "        nb_words_in_matrix = nb_words_in_matrix + 1\n",
    "        \n",
    "print(\"added %d words in the embedding matrix\" % nb_words_in_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a layer with pre-trained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding_layer = Embedding(\n",
    "    MAX_NB_WORDS, EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A model with pre-trained Embeddings\n",
    "\n",
    "Now we can build a model with pre-trained embeddings. We will use the same architecture as before, but we will use the `pretrained_embedding_layer` as the first layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    # Add the pre-defined and pre-trained embedding layer\n",
    "    pretrained_embedding_layer,\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(N_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Set the embedding layer's trainable attribute to False to not fine-tune the embeddings - you can try to change this\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_split=0.1,\n",
    "          epochs=15, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reality check\n",
    "\n",
    "On small/medium datasets (few 10,000s) of reasonably large documents (e.g. more than a few paragraphs), simpler classification methods usually perform better, and are much more efficient to train and use. Here are two resources to go further, if you are curious:\n",
    "- Naive Bayes approach, using scikit-learn [http://scikit-learn.org/stable/datasets/twenty_newsgroups.html](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)\n",
    "- Alec Radford (OpenAI) gave a very interesting presentation, showing that you need a VERY large dataset to have real gains from GRU/LSTM in text classification [https://www.slideshare.net/odsc/alec-radfordodsc-presentation](https://www.slideshare.net/odsc/alec-radfordodsc-presentation)\n",
    "\n",
    "Training deep architectures from random init on text classification is usually a waste of time.\n",
    "\n",
    "However, when looking at features, one can see that classification using simple methods isn't very robust, and won't generalize well to slightly different domains (e.g. forum posts => emails)\n",
    "\n",
    "Nowadays, the strategy would be to use pre-trained deep network (BERT) to extract features and fit a linear classifer on top of this. This is especially useful when classifying short texts (e.g. one or a few sentences) as this kind of tasks can be very sensitive to understanding the meaning resulting from intra-sentence interactions between words. The next session on attentional mechanisms and pre-trained transformer-based word models will explain this in more details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSI_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
